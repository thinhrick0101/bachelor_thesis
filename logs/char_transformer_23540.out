=== Training BPE Tokenizer ===



Tokenizer trained and saved.
Tokens: ['[CLS]', 'By', 'te', 'P', 'air', 'En', 'coding', 'is', 'aw', 'es', 'ome', '!', '[SEP]']
Token IDs: [2, 7453, 6282, 52, 6512, 6364, 17400, 6076, 6343, 6073, 6237, 5, 3]
Loading training data from data/enwik8
Training data size: 99,621,832 characters

Initializing tokenizer with vocabulary size 8,000

Training tokenizer...
Analyzing character n-grams...
Initial vocabulary size: 6069 tokens
Character n-gram types: 383,078
Analyzing token coverage...
Converting text to base tokens...
Processing batch 1/100
Processing batch 2/100
Processing batch 3/100
Processing batch 4/100
Processing batch 5/100
Processing batch 6/100
Processing batch 7/100
Processing batch 8/100
Processing batch 9/100
Processing batch 10/100
Processing batch 11/100
Processing batch 12/100
Processing batch 13/100
Processing batch 14/100
Processing batch 15/100
Processing batch 16/100
Processing batch 17/100
Processing batch 18/100
Processing batch 19/100
Processing batch 20/100
Processing batch 21/100
Processing batch 22/100
Processing batch 23/100
Processing batch 24/100
Processing batch 25/100
Processing batch 26/100
Processing batch 27/100
Processing batch 28/100
Processing batch 29/100
Processing batch 30/100
Processing batch 31/100
Processing batch 32/100
Processing batch 33/100
Processing batch 34/100
Processing batch 35/100
Processing batch 36/100
Processing batch 37/100
Processing batch 38/100
Processing batch 39/100
Processing batch 40/100
Processing batch 41/100
Processing batch 42/100
Processing batch 43/100
Processing batch 44/100
Processing batch 45/100
Processing batch 46/100
Processing batch 47/100
Processing batch 48/100
Processing batch 49/100
Processing batch 50/100
Processing batch 51/100
Processing batch 52/100
Processing batch 53/100
Processing batch 54/100
Processing batch 55/100
Processing batch 56/100
Processing batch 57/100
Processing batch 58/100
Processing batch 59/100
Processing batch 60/100
Processing batch 61/100
Processing batch 62/100
Processing batch 63/100
Processing batch 64/100
Processing batch 65/100
Processing batch 66/100
Processing batch 67/100
Processing batch 68/100
Processing batch 69/100
Processing batch 70/100
Processing batch 71/100
Processing batch 72/100
Processing batch 73/100
Processing batch 74/100
Processing batch 75/100
Processing batch 76/100
Processing batch 77/100
Processing batch 78/100
Processing batch 79/100
Processing batch 80/100
Processing batch 81/100
Processing batch 82/100
Processing batch 83/100
Processing batch 84/100
Processing batch 85/100
Processing batch 86/100
Processing batch 87/100
Processing batch 88/100
Processing batch 89/100
Processing batch 90/100
Processing batch 91/100
Processing batch 92/100
Processing batch 93/100
Processing batch 94/100
Processing batch 95/100
Processing batch 96/100
Processing batch 97/100
Processing batch 98/100
Processing batch 99/100
Processing batch 100/100
Converting words to token IDs...
Initial token count: 84,969,592
Training BPE tokenizer to add 1,931 new tokens
