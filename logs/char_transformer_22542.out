Loading data...
Loading data from data/enwik8
Data loaded: 99621832 characters
Limiting data to first 10000000 characters for training
Vocabulary size: 2102 characters
Creating batches...
Created 274 training batches and 30 validation batches
Model Parameters: 64,624,719 trainable out of 64,624,719 total

=== Training Enhanced Character Transformer Model ===
Training on device: cuda
Using mixed precision training (FP16)
Using gradient accumulation with 4 steps
Effective batch size: 256
Epoch 1/100, Batch 10/274, Loss: 7.6302
Epoch 1/100, Batch 20/274, Loss: 7.4753
Epoch 1/100, Batch 30/274, Loss: 7.0559
Epoch 1/100, Batch 40/274, Loss: 6.7923
Epoch 1/100, Batch 50/274, Loss: 6.3773
Epoch 1/100, Batch 60/274, Loss: 6.1587
Epoch 1/100, Batch 70/274, Loss: 5.9591
Epoch 1/100, Batch 80/274, Loss: 5.8663
Epoch 1/100, Batch 90/274, Loss: 5.7346
Epoch 1/100, Batch 100/274, Loss: 5.6383
Epoch 1/100, Batch 110/274, Loss: 5.4649
Epoch 1/100, Batch 120/274, Loss: 5.4438
Epoch 1/100, Batch 130/274, Loss: 5.2885
Epoch 1/100, Batch 140/274, Loss: 5.2157
Epoch 1/100, Batch 150/274, Loss: 5.0732
Epoch 1/100, Batch 160/274, Loss: 5.0117
Epoch 1/100, Batch 170/274, Loss: 4.7847
Epoch 1/100, Batch 180/274, Loss: 4.6981
Epoch 1/100, Batch 190/274, Loss: 4.5531
Epoch 1/100, Batch 200/274, Loss: 4.4330
Epoch 1/100, Batch 210/274, Loss: 4.2829
Epoch 1/100, Batch 220/274, Loss: 4.2661
Epoch 1/100, Batch 230/274, Loss: 4.0930
Epoch 1/100, Batch 240/274, Loss: 4.0231
Epoch 1/100, Batch 250/274, Loss: 3.9659
Epoch 1/100, Batch 260/274, Loss: 3.9273
Epoch 1/100, Batch 270/274, Loss: 3.8030
New best model with validation loss: 3.8267, perplexity: 45.91
Epoch 1/100, Loss: 5.3374, Perplexity: 207.97, Val Loss: 3.8267, Val Perplexity: 45.91, Time: 366.38s
Epoch 2/100, Batch 10/274, Loss: 3.8054
Epoch 2/100, Batch 20/274, Loss: 3.7486
Epoch 2/100, Batch 30/274, Loss: 3.7677
Epoch 2/100, Batch 40/274, Loss: 3.7450
Epoch 2/100, Batch 50/274, Loss: 3.7022
Epoch 2/100, Batch 60/274, Loss: 3.7000
Epoch 2/100, Batch 70/274, Loss: 3.6799
Epoch 2/100, Batch 80/274, Loss: 3.6826
Epoch 2/100, Batch 90/274, Loss: 3.6497
Epoch 2/100, Batch 100/274, Loss: 3.6280
Epoch 2/100, Batch 110/274, Loss: 3.5919
Epoch 2/100, Batch 120/274, Loss: 3.6552
Epoch 2/100, Batch 130/274, Loss: 3.5940
Epoch 2/100, Batch 140/274, Loss: 3.6093
Epoch 2/100, Batch 150/274, Loss: 3.5644
Epoch 2/100, Batch 160/274, Loss: 3.6110
Epoch 2/100, Batch 170/274, Loss: 3.5589
Epoch 2/100, Batch 180/274, Loss: 3.5407
Epoch 2/100, Batch 190/274, Loss: 3.5405
Epoch 2/100, Batch 200/274, Loss: 3.5127
Epoch 2/100, Batch 210/274, Loss: 3.4930
Epoch 2/100, Batch 220/274, Loss: 3.5338
Epoch 2/100, Batch 230/274, Loss: 3.5170
Epoch 2/100, Batch 240/274, Loss: 3.5260
Epoch 2/100, Batch 250/274, Loss: 3.5304
Epoch 2/100, Batch 260/274, Loss: 3.5316
Epoch 2/100, Batch 270/274, Loss: 3.4918
New best model with validation loss: 3.4845, perplexity: 32.61
Epoch 2/100, Loss: 3.6136, Perplexity: 37.10, Val Loss: 3.4845, Val Perplexity: 32.61, Time: 369.11s
Epoch 3/100, Batch 10/274, Loss: 3.5415
Epoch 3/100, Batch 20/274, Loss: 3.5023
Epoch 3/100, Batch 30/274, Loss: 3.5147
Epoch 3/100, Batch 40/274, Loss: 3.4994
Epoch 3/100, Batch 50/274, Loss: 3.4802
Epoch 3/100, Batch 60/274, Loss: 3.5162
Epoch 3/100, Batch 70/274, Loss: 3.4843
Epoch 3/100, Batch 80/274, Loss: 3.4880
Epoch 3/100, Batch 90/274, Loss: 3.4402
Epoch 3/100, Batch 100/274, Loss: 3.4552
Epoch 3/100, Batch 110/274, Loss: 3.4588
Epoch 3/100, Batch 120/274, Loss: 3.4964
Epoch 3/100, Batch 130/274, Loss: 3.4392
Epoch 3/100, Batch 140/274, Loss: 3.4409
Epoch 3/100, Batch 150/274, Loss: 3.4339
Epoch 3/100, Batch 160/274, Loss: 3.4685
Epoch 3/100, Batch 170/274, Loss: 3.4338
Epoch 3/100, Batch 180/274, Loss: 3.4235
Epoch 3/100, Batch 190/274, Loss: 3.4373
Epoch 3/100, Batch 200/274, Loss: 3.4133
Epoch 3/100, Batch 210/274, Loss: 3.4145
Epoch 3/100, Batch 220/274, Loss: 3.4217
Epoch 3/100, Batch 230/274, Loss: 3.4275
Epoch 3/100, Batch 240/274, Loss: 3.4308
Epoch 3/100, Batch 250/274, Loss: 3.4317
Epoch 3/100, Batch 260/274, Loss: 3.4322
Epoch 3/100, Batch 270/274, Loss: 3.3936
New best model with validation loss: 3.3690, perplexity: 29.05
Epoch 3/100, Loss: 3.4561, Perplexity: 31.69, Val Loss: 3.3690, Val Perplexity: 29.05, Time: 369.04s
Epoch 4/100, Batch 10/274, Loss: 3.4184
Epoch 4/100, Batch 20/274, Loss: 3.3948
Epoch 4/100, Batch 30/274, Loss: 3.4341
Epoch 4/100, Batch 40/274, Loss: 3.4118
Epoch 4/100, Batch 50/274, Loss: 3.3715
Epoch 4/100, Batch 60/274, Loss: 3.4143
Epoch 4/100, Batch 70/274, Loss: 3.3748
Epoch 4/100, Batch 80/274, Loss: 3.3787
Epoch 4/100, Batch 90/274, Loss: 3.3251
Epoch 4/100, Batch 100/274, Loss: 3.3442
Epoch 4/100, Batch 110/274, Loss: 3.3564
Epoch 4/100, Batch 120/274, Loss: 3.3856
Epoch 4/100, Batch 130/274, Loss: 3.3404
Epoch 4/100, Batch 140/274, Loss: 3.3397
Epoch 4/100, Batch 150/274, Loss: 3.3424
Epoch 4/100, Batch 160/274, Loss: 3.3669
Epoch 4/100, Batch 170/274, Loss: 3.3457
Epoch 4/100, Batch 180/274, Loss: 3.3438
Epoch 4/100, Batch 190/274, Loss: 3.3506
Epoch 4/100, Batch 200/274, Loss: 3.3197
Epoch 4/100, Batch 210/274, Loss: 3.3204
Epoch 4/100, Batch 220/274, Loss: 3.3262
Epoch 4/100, Batch 230/274, Loss: 3.3284
Epoch 4/100, Batch 240/274, Loss: 3.3347
Epoch 4/100, Batch 250/274, Loss: 3.3314
Epoch 4/100, Batch 260/274, Loss: 3.3449
Epoch 4/100, Batch 270/274, Loss: 3.3038
New best model with validation loss: 3.2573, perplexity: 25.98
Epoch 4/100, Loss: 3.3559, Perplexity: 28.67, Val Loss: 3.2573, Val Perplexity: 25.98, Time: 368.92s
Epoch 5/100, Batch 10/274, Loss: 3.3288
Epoch 5/100, Batch 20/274, Loss: 3.3234
Epoch 5/100, Batch 30/274, Loss: 3.3105
Epoch 5/100, Batch 40/274, Loss: 3.3123
Epoch 5/100, Batch 50/274, Loss: 3.2692
Epoch 5/100, Batch 60/274, Loss: 3.3212
Epoch 5/100, Batch 70/274, Loss: 3.2773
Epoch 5/100, Batch 80/274, Loss: 3.2880
Epoch 5/100, Batch 90/274, Loss: 3.2230
Epoch 5/100, Batch 100/274, Loss: 3.2448
Epoch 5/100, Batch 110/274, Loss: 3.2654
Epoch 5/100, Batch 120/274, Loss: 3.2883
Epoch 5/100, Batch 130/274, Loss: 3.2443
Epoch 5/100, Batch 140/274, Loss: 3.2396
Epoch 5/100, Batch 150/274, Loss: 3.2372
Epoch 5/100, Batch 160/274, Loss: 3.3233
Epoch 5/100, Batch 170/274, Loss: 3.2592
Epoch 5/100, Batch 180/274, Loss: 3.2463
Epoch 5/100, Batch 190/274, Loss: 3.2661
Epoch 5/100, Batch 200/274, Loss: 3.2317
Epoch 5/100, Batch 210/274, Loss: 3.2319
Epoch 5/100, Batch 220/274, Loss: 3.2397
Epoch 5/100, Batch 230/274, Loss: 3.2437
Epoch 5/100, Batch 240/274, Loss: 3.2527
Epoch 5/100, Batch 250/274, Loss: 3.2479
Epoch 5/100, Batch 260/274, Loss: 3.2514
Epoch 5/100, Batch 270/274, Loss: 3.2125
New best model with validation loss: 3.1410, perplexity: 23.13
Epoch 5/100, Loss: 3.2630, Perplexity: 26.13, Val Loss: 3.1410, Val Perplexity: 23.13, Time: 368.22s
Epoch 6/100, Batch 10/274, Loss: 3.2435
Epoch 6/100, Batch 20/274, Loss: 3.2217
Epoch 6/100, Batch 30/274, Loss: 3.2080
Epoch 6/100, Batch 40/274, Loss: 3.2148
Epoch 6/100, Batch 50/274, Loss: 3.1684
Epoch 6/100, Batch 60/274, Loss: 3.2248
Epoch 6/100, Batch 70/274, Loss: 3.1731
Epoch 6/100, Batch 80/274, Loss: 3.1809
Epoch 6/100, Batch 90/274, Loss: 3.1109
Epoch 6/100, Batch 100/274, Loss: 3.1441
Epoch 6/100, Batch 110/274, Loss: 3.1698
Epoch 6/100, Batch 120/274, Loss: 3.1901
Epoch 6/100, Batch 130/274, Loss: 3.1447
Epoch 6/100, Batch 140/274, Loss: 3.1432
Epoch 6/100, Batch 150/274, Loss: 3.1310
Epoch 6/100, Batch 160/274, Loss: 3.1789
Epoch 6/100, Batch 170/274, Loss: 3.1717
Epoch 6/100, Batch 180/274, Loss: 3.1684
Epoch 6/100, Batch 190/274, Loss: 3.1727
Epoch 6/100, Batch 200/274, Loss: 3.1411
Epoch 6/100, Batch 210/274, Loss: 3.1439
Epoch 6/100, Batch 220/274, Loss: 3.1543
Epoch 6/100, Batch 230/274, Loss: 3.1513
Epoch 6/100, Batch 240/274, Loss: 3.1603
Epoch 6/100, Batch 250/274, Loss: 3.1404
Epoch 6/100, Batch 260/274, Loss: 3.1659
Epoch 6/100, Batch 270/274, Loss: 3.1469
New best model with validation loss: 3.0509, perplexity: 21.13
Epoch 6/100, Loss: 3.1675, Perplexity: 23.75, Val Loss: 3.0509, Val Perplexity: 21.13, Time: 369.68s
Epoch 7/100, Batch 10/274, Loss: 3.1450
Epoch 7/100, Batch 20/274, Loss: 3.1478
Epoch 7/100, Batch 30/274, Loss: 3.1252
Epoch 7/100, Batch 40/274, Loss: 3.1171
Epoch 7/100, Batch 50/274, Loss: 3.0867
Epoch 7/100, Batch 60/274, Loss: 3.1434
Epoch 7/100, Batch 70/274, Loss: 3.0868
Epoch 7/100, Batch 80/274, Loss: 3.0968
Epoch 7/100, Batch 90/274, Loss: 3.0213
Epoch 7/100, Batch 100/274, Loss: 3.0567
Epoch 7/100, Batch 110/274, Loss: 3.1033
Epoch 7/100, Batch 120/274, Loss: 3.0984
Epoch 7/100, Batch 130/274, Loss: 3.0566
Epoch 7/100, Batch 140/274, Loss: 3.0670
Epoch 7/100, Batch 150/274, Loss: 3.0593
Epoch 7/100, Batch 160/274, Loss: 3.0950
Epoch 7/100, Batch 170/274, Loss: 3.0949
Epoch 7/100, Batch 180/274, Loss: 3.0794
Epoch 7/100, Batch 190/274, Loss: 3.1016
Epoch 7/100, Batch 200/274, Loss: 3.0597
Epoch 7/100, Batch 210/274, Loss: 3.0761
Epoch 7/100, Batch 220/274, Loss: 3.0815
Epoch 7/100, Batch 230/274, Loss: 3.0766
Epoch 7/100, Batch 240/274, Loss: 3.0954
Epoch 7/100, Batch 250/274, Loss: 3.0736
Epoch 7/100, Batch 260/274, Loss: 3.0888
Epoch 7/100, Batch 270/274, Loss: 3.0488
New best model with validation loss: 2.9674, perplexity: 19.44
Epoch 7/100, Loss: 3.0880, Perplexity: 21.93, Val Loss: 2.9674, Val Perplexity: 19.44, Time: 370.04s
Epoch 8/100, Batch 10/274, Loss: 3.1408
Epoch 8/100, Batch 20/274, Loss: 3.0663
Epoch 8/100, Batch 30/274, Loss: 3.0463
Epoch 8/100, Batch 40/274, Loss: 3.0370
Epoch 8/100, Batch 50/274, Loss: 3.0131
Epoch 8/100, Batch 60/274, Loss: 3.1290
Epoch 8/100, Batch 70/274, Loss: 3.0143
Epoch 8/100, Batch 80/274, Loss: 3.0238
Epoch 8/100, Batch 90/274, Loss: 2.9542
Epoch 8/100, Batch 100/274, Loss: 2.9923
Epoch 8/100, Batch 110/274, Loss: 3.0283
Epoch 8/100, Batch 120/274, Loss: 3.0396
Epoch 8/100, Batch 130/274, Loss: 2.9901
Epoch 8/100, Batch 140/274, Loss: 2.9946
Epoch 8/100, Batch 150/274, Loss: 2.9779
Epoch 8/100, Batch 160/274, Loss: 3.0218
Epoch 8/100, Batch 170/274, Loss: 3.0328
Epoch 8/100, Batch 180/274, Loss: 3.0235
Epoch 8/100, Batch 190/274, Loss: 3.0319
Epoch 8/100, Batch 200/274, Loss: 2.9918
Epoch 8/100, Batch 210/274, Loss: 3.0046
Epoch 8/100, Batch 220/274, Loss: 3.0078
Epoch 8/100, Batch 230/274, Loss: 3.0191
Epoch 8/100, Batch 240/274, Loss: 3.0356
Epoch 8/100, Batch 250/274, Loss: 3.0138
Epoch 8/100, Batch 260/274, Loss: 3.0317
Epoch 8/100, Batch 270/274, Loss: 2.9875
New best model with validation loss: 2.9077, perplexity: 18.31
Epoch 8/100, Loss: 3.0202, Perplexity: 20.50, Val Loss: 2.9077, Val Perplexity: 18.31, Time: 368.84s
Epoch 9/100, Batch 10/274, Loss: 3.0100
Epoch 9/100, Batch 20/274, Loss: 3.0006
Epoch 9/100, Batch 30/274, Loss: 2.9836
Epoch 9/100, Batch 40/274, Loss: 2.9853
Epoch 9/100, Batch 50/274, Loss: 2.9469
Epoch 9/100, Batch 60/274, Loss: 3.0199
Epoch 9/100, Batch 70/274, Loss: 2.9525
Epoch 9/100, Batch 80/274, Loss: 2.9668
Epoch 9/100, Batch 90/274, Loss: 2.8915
Epoch 9/100, Batch 100/274, Loss: 2.9247
Epoch 9/100, Batch 110/274, Loss: 2.9700
Epoch 9/100, Batch 120/274, Loss: 2.9768
Epoch 9/100, Batch 130/274, Loss: 2.9228
Epoch 9/100, Batch 140/274, Loss: 2.9332
Epoch 9/100, Batch 150/274, Loss: 2.9172
Epoch 9/100, Batch 160/274, Loss: 2.9664
Epoch 9/100, Batch 170/274, Loss: 2.9671
Epoch 9/100, Batch 180/274, Loss: 2.9574
Epoch 9/100, Batch 190/274, Loss: 2.9790
Epoch 9/100, Batch 200/274, Loss: 2.9325
Epoch 9/100, Batch 210/274, Loss: 2.9548
Epoch 9/100, Batch 220/274, Loss: 2.9440
Epoch 9/100, Batch 230/274, Loss: 2.9553
Epoch 9/100, Batch 240/274, Loss: 2.9768
Epoch 9/100, Batch 250/274, Loss: 2.9407
Epoch 9/100, Batch 260/274, Loss: 2.9726
Epoch 9/100, Batch 270/274, Loss: 2.9383
New best model with validation loss: 2.8479, perplexity: 17.25
Epoch 9/100, Loss: 2.9600, Perplexity: 19.30, Val Loss: 2.8479, Val Perplexity: 17.25, Time: 370.27s
Epoch 10/100, Batch 10/274, Loss: 2.9819
Epoch 10/100, Batch 20/274, Loss: 2.9452
Epoch 10/100, Batch 30/274, Loss: 2.9296
Epoch 10/100, Batch 40/274, Loss: 2.9313
Epoch 10/100, Batch 50/274, Loss: 2.9005
Epoch 10/100, Batch 60/274, Loss: 2.9601
Epoch 10/100, Batch 70/274, Loss: 2.9090
Epoch 10/100, Batch 80/274, Loss: 2.9103
Epoch 10/100, Batch 90/274, Loss: 2.8421
Epoch 10/100, Batch 100/274, Loss: 2.8739
Epoch 10/100, Batch 110/274, Loss: 2.9134
Epoch 10/100, Batch 120/274, Loss: 2.9057
Epoch 10/100, Batch 130/274, Loss: 2.8738
Epoch 10/100, Batch 140/274, Loss: 2.8761
Epoch 10/100, Batch 150/274, Loss: 2.8597
Epoch 10/100, Batch 160/274, Loss: 2.9067
Epoch 10/100, Batch 170/274, Loss: 2.9236
Epoch 10/100, Batch 180/274, Loss: 2.9829
Epoch 10/100, Batch 190/274, Loss: 2.9327
Epoch 10/100, Batch 200/274, Loss: 2.8842
Epoch 10/100, Batch 210/274, Loss: 2.8956
Epoch 10/100, Batch 220/274, Loss: 2.8856
Epoch 10/100, Batch 230/274, Loss: 2.9104
Epoch 10/100, Batch 240/274, Loss: 2.9331
Epoch 10/100, Batch 250/274, Loss: 2.9016
Epoch 10/100, Batch 260/274, Loss: 2.9260
Epoch 10/100, Batch 270/274, Loss: 2.8844
New best model with validation loss: 2.8011, perplexity: 16.46
Epoch 10/100, Loss: 2.9074, Perplexity: 18.31, Val Loss: 2.8011, Val Perplexity: 16.46, Time: 369.33s
Epoch 11/100, Batch 10/274, Loss: 2.9113
Epoch 11/100, Batch 20/274, Loss: 2.8927
Epoch 11/100, Batch 30/274, Loss: 2.8722
Epoch 11/100, Batch 40/274, Loss: 2.8758
Epoch 11/100, Batch 50/274, Loss: 2.8452
Epoch 11/100, Batch 60/274, Loss: 2.9217
Epoch 11/100, Batch 70/274, Loss: 2.8513
Epoch 11/100, Batch 80/274, Loss: 2.8593
Epoch 11/100, Batch 90/274, Loss: 2.7833
Epoch 11/100, Batch 100/274, Loss: 2.8247
Epoch 11/100, Batch 110/274, Loss: 2.8725
Epoch 11/100, Batch 120/274, Loss: 2.8566
Epoch 11/100, Batch 130/274, Loss: 2.8272
Epoch 11/100, Batch 140/274, Loss: 2.8253
Epoch 11/100, Batch 150/274, Loss: 2.8071
Epoch 11/100, Batch 160/274, Loss: 2.8505
Epoch 11/100, Batch 170/274, Loss: 2.8707
Epoch 11/100, Batch 180/274, Loss: 2.8732
Epoch 11/100, Batch 190/274, Loss: 2.8779
Epoch 11/100, Batch 200/274, Loss: 2.8311
Epoch 11/100, Batch 210/274, Loss: 2.8635
Epoch 11/100, Batch 220/274, Loss: 2.8405
Epoch 11/100, Batch 230/274, Loss: 2.8572
Epoch 11/100, Batch 240/274, Loss: 2.8929
Epoch 11/100, Batch 250/274, Loss: 2.8596
Epoch 11/100, Batch 260/274, Loss: 2.8766
Epoch 11/100, Batch 270/274, Loss: 2.8421
New best model with validation loss: 2.7529, perplexity: 15.69
Epoch 11/100, Loss: 2.8588, Perplexity: 17.44, Val Loss: 2.7529, Val Perplexity: 15.69, Time: 371.22s
Epoch 12/100, Batch 10/274, Loss: 2.9500
Epoch 12/100, Batch 20/274, Loss: 2.8447
Epoch 12/100, Batch 30/274, Loss: 2.8251
Epoch 12/100, Batch 40/274, Loss: 2.8234
Epoch 12/100, Batch 50/274, Loss: 2.8004
Epoch 12/100, Batch 60/274, Loss: 2.8648
Epoch 12/100, Batch 70/274, Loss: 2.8045
Epoch 12/100, Batch 80/274, Loss: 2.8159
Epoch 12/100, Batch 90/274, Loss: 2.7432
Epoch 12/100, Batch 100/274, Loss: 2.7860
Epoch 12/100, Batch 110/274, Loss: 2.8140
Epoch 12/100, Batch 120/274, Loss: 2.8128
Epoch 12/100, Batch 130/274, Loss: 2.7758
Epoch 12/100, Batch 140/274, Loss: 2.7879
Epoch 12/100, Batch 150/274, Loss: 2.7685
Epoch 12/100, Batch 160/274, Loss: 2.8020
Epoch 12/100, Batch 170/274, Loss: 2.8383
Epoch 12/100, Batch 180/274, Loss: 2.8377
Epoch 12/100, Batch 190/274, Loss: 2.8482
Epoch 12/100, Batch 200/274, Loss: 2.7884
Epoch 12/100, Batch 210/274, Loss: 2.8046
Epoch 12/100, Batch 220/274, Loss: 2.7800
Epoch 12/100, Batch 230/274, Loss: 2.8279
Epoch 12/100, Batch 240/274, Loss: 2.8481
Epoch 12/100, Batch 250/274, Loss: 2.8141
Epoch 12/100, Batch 260/274, Loss: 2.8423
Epoch 12/100, Batch 270/274, Loss: 2.7922
New best model with validation loss: 2.7090, perplexity: 15.01
Epoch 12/100, Loss: 2.8141, Perplexity: 16.68, Val Loss: 2.7090, Val Perplexity: 15.01, Time: 369.94s
Epoch 13/100, Batch 10/274, Loss: 2.8253
Epoch 13/100, Batch 20/274, Loss: 2.7982
Epoch 13/100, Batch 30/274, Loss: 2.7790
Epoch 13/100, Batch 40/274, Loss: 2.7747
Epoch 13/100, Batch 50/274, Loss: 2.7626
Epoch 13/100, Batch 60/274, Loss: 2.8251
Epoch 13/100, Batch 70/274, Loss: 2.7668
Epoch 13/100, Batch 80/274, Loss: 2.7764
Epoch 13/100, Batch 90/274, Loss: 2.6961
Epoch 13/100, Batch 100/274, Loss: 2.7352
Epoch 13/100, Batch 110/274, Loss: 2.7748
Epoch 13/100, Batch 120/274, Loss: 2.7596
Epoch 13/100, Batch 130/274, Loss: 2.7352
Epoch 13/100, Batch 140/274, Loss: 2.7466
Epoch 13/100, Batch 150/274, Loss: 2.7263
Epoch 13/100, Batch 160/274, Loss: 2.7545
Epoch 13/100, Batch 170/274, Loss: 2.8014
Epoch 13/100, Batch 180/274, Loss: 2.7695
Epoch 13/100, Batch 190/274, Loss: 2.7836
Epoch 13/100, Batch 200/274, Loss: 2.7376
Epoch 13/100, Batch 210/274, Loss: 2.7616
Epoch 13/100, Batch 220/274, Loss: 2.7428
Epoch 13/100, Batch 230/274, Loss: 2.7839
Epoch 13/100, Batch 240/274, Loss: 2.8031
Epoch 13/100, Batch 250/274, Loss: 2.7745
Epoch 13/100, Batch 260/274, Loss: 2.8128
Epoch 13/100, Batch 270/274, Loss: 2.7396
New best model with validation loss: 2.6654, perplexity: 14.37
Epoch 13/100, Loss: 2.7696, Perplexity: 15.95, Val Loss: 2.6654, Val Perplexity: 14.37, Time: 370.10s
Epoch 14/100, Batch 10/274, Loss: 2.7930
Epoch 14/100, Batch 20/274, Loss: 2.7528
Epoch 14/100, Batch 30/274, Loss: 2.7341
Epoch 14/100, Batch 40/274, Loss: 2.7444
Epoch 14/100, Batch 50/274, Loss: 2.7154
Epoch 14/100, Batch 60/274, Loss: 2.7747
Epoch 14/100, Batch 70/274, Loss: 2.7307
Epoch 14/100, Batch 80/274, Loss: 2.7165
Epoch 14/100, Batch 90/274, Loss: 2.6579
Epoch 14/100, Batch 100/274, Loss: 2.6935
Epoch 14/100, Batch 110/274, Loss: 2.7211
Epoch 14/100, Batch 120/274, Loss: 2.7213
Epoch 14/100, Batch 130/274, Loss: 2.6904
Epoch 14/100, Batch 140/274, Loss: 2.7027
Epoch 14/100, Batch 150/274, Loss: 2.6845
Epoch 14/100, Batch 160/274, Loss: 2.7146
Epoch 14/100, Batch 170/274, Loss: 2.7505
Epoch 14/100, Batch 180/274, Loss: 2.7299
Epoch 14/100, Batch 190/274, Loss: 2.7507
Epoch 14/100, Batch 200/274, Loss: 2.7027
Epoch 14/100, Batch 210/274, Loss: 2.7245
Epoch 14/100, Batch 220/274, Loss: 2.6935
Epoch 14/100, Batch 230/274, Loss: 2.7396
Epoch 14/100, Batch 240/274, Loss: 2.7651
Epoch 14/100, Batch 250/274, Loss: 2.7336
Epoch 14/100, Batch 260/274, Loss: 2.7490
Epoch 14/100, Batch 270/274, Loss: 2.7039
New best model with validation loss: 2.6099, perplexity: 13.60
Epoch 14/100, Loss: 2.7254, Perplexity: 15.26, Val Loss: 2.6099, Val Perplexity: 13.60, Time: 370.53s
Epoch 15/100, Batch 10/274, Loss: 2.7388
Epoch 15/100, Batch 20/274, Loss: 2.7375
Epoch 15/100, Batch 30/274, Loss: 2.7228
Epoch 15/100, Batch 40/274, Loss: 2.7015
Epoch 15/100, Batch 50/274, Loss: 2.6787
Epoch 15/100, Batch 60/274, Loss: 2.7487
Epoch 15/100, Batch 70/274, Loss: 2.6851
Epoch 15/100, Batch 80/274, Loss: 2.6715
Epoch 15/100, Batch 90/274, Loss: 2.6145
Epoch 15/100, Batch 100/274, Loss: 2.6532
Epoch 15/100, Batch 110/274, Loss: 2.6847
Epoch 15/100, Batch 120/274, Loss: 2.6774
Epoch 15/100, Batch 130/274, Loss: 2.6348
Epoch 15/100, Batch 140/274, Loss: 2.6608
Epoch 15/100, Batch 150/274, Loss: 2.6348
Epoch 15/100, Batch 160/274, Loss: 2.6668
Epoch 15/100, Batch 170/274, Loss: 2.6874
Epoch 15/100, Batch 180/274, Loss: 2.6939
Epoch 15/100, Batch 190/274, Loss: 2.7111
Epoch 15/100, Batch 200/274, Loss: 2.6553
Epoch 15/100, Batch 210/274, Loss: 2.6821
Epoch 15/100, Batch 220/274, Loss: 2.6567
Epoch 15/100, Batch 230/274, Loss: 2.7328
Epoch 15/100, Batch 240/274, Loss: 2.7799
Epoch 15/100, Batch 250/274, Loss: 2.6980
Epoch 15/100, Batch 260/274, Loss: 2.7056
Epoch 15/100, Batch 270/274, Loss: 2.6676
New best model with validation loss: 2.5673, perplexity: 13.03
Epoch 15/100, Loss: 2.6847, Perplexity: 14.65, Val Loss: 2.5673, Val Perplexity: 13.03, Time: 369.49s
Epoch 16/100, Batch 10/274, Loss: 2.6916
Epoch 16/100, Batch 20/274, Loss: 2.6666
Epoch 16/100, Batch 30/274, Loss: 2.6433
Epoch 16/100, Batch 40/274, Loss: 2.6537
Epoch 16/100, Batch 50/274, Loss: 2.6249
Epoch 16/100, Batch 60/274, Loss: 2.6979
Epoch 16/100, Batch 70/274, Loss: 2.6387
Epoch 16/100, Batch 80/274, Loss: 2.6378
Epoch 16/100, Batch 90/274, Loss: 2.5951
Epoch 16/100, Batch 100/274, Loss: 2.6085
Epoch 16/100, Batch 110/274, Loss: 2.6455
Epoch 16/100, Batch 120/274, Loss: 2.6818
Epoch 16/100, Batch 130/274, Loss: 2.6033
Epoch 16/100, Batch 140/274, Loss: 2.6453
Epoch 16/100, Batch 150/274, Loss: 2.5906
Epoch 16/100, Batch 160/274, Loss: 2.6443
Epoch 16/100, Batch 170/274, Loss: 2.6706
Epoch 16/100, Batch 180/274, Loss: 2.6553
Epoch 16/100, Batch 190/274, Loss: 2.6705
Epoch 16/100, Batch 200/274, Loss: 2.6160
Epoch 16/100, Batch 210/274, Loss: 2.6388
Epoch 16/100, Batch 220/274, Loss: 2.6329
Epoch 16/100, Batch 230/274, Loss: 2.6639
Epoch 16/100, Batch 240/274, Loss: 2.6731
Epoch 16/100, Batch 250/274, Loss: 2.6736
Epoch 16/100, Batch 260/274, Loss: 2.6924
Epoch 16/100, Batch 270/274, Loss: 2.6283
New best model with validation loss: 2.5179, perplexity: 12.40
Epoch 16/100, Loss: 2.6481, Perplexity: 14.13, Val Loss: 2.5179, Val Perplexity: 12.40, Time: 367.74s
Epoch 17/100, Batch 10/274, Loss: 2.6653
Epoch 17/100, Batch 20/274, Loss: 2.6331
Epoch 17/100, Batch 30/274, Loss: 2.6222
Epoch 17/100, Batch 40/274, Loss: 2.6108
Epoch 17/100, Batch 50/274, Loss: 2.5893
Epoch 17/100, Batch 60/274, Loss: 2.6506
Epoch 17/100, Batch 70/274, Loss: 2.6349
Epoch 17/100, Batch 80/274, Loss: 2.5971
Epoch 17/100, Batch 90/274, Loss: 2.5547
Epoch 17/100, Batch 100/274, Loss: 2.5698
Epoch 17/100, Batch 110/274, Loss: 2.6051
Epoch 17/100, Batch 120/274, Loss: 2.7192
Epoch 17/100, Batch 130/274, Loss: 2.5673
Epoch 17/100, Batch 140/274, Loss: 2.5895
Epoch 17/100, Batch 150/274, Loss: 2.5531
Epoch 17/100, Batch 160/274, Loss: 2.5990
Epoch 17/100, Batch 170/274, Loss: 2.6198
Epoch 17/100, Batch 180/274, Loss: 2.6154
Epoch 17/100, Batch 190/274, Loss: 2.6357
Epoch 17/100, Batch 200/274, Loss: 2.5778
Epoch 17/100, Batch 210/274, Loss: 2.6148
Epoch 17/100, Batch 220/274, Loss: 2.5771
Epoch 17/100, Batch 230/274, Loss: 2.6263
Epoch 17/100, Batch 240/274, Loss: 2.6404
Epoch 17/100, Batch 250/274, Loss: 2.6279
Epoch 17/100, Batch 260/274, Loss: 2.6318
Epoch 17/100, Batch 270/274, Loss: 2.6302
New best model with validation loss: 2.4871, perplexity: 12.03
Epoch 17/100, Loss: 2.6100, Perplexity: 13.60, Val Loss: 2.4871, Val Perplexity: 12.03, Time: 370.81s
Epoch 18/100, Batch 10/274, Loss: 2.6276
Epoch 18/100, Batch 20/274, Loss: 2.6229
Epoch 18/100, Batch 30/274, Loss: 2.5719
Epoch 18/100, Batch 40/274, Loss: 2.5825
Epoch 18/100, Batch 50/274, Loss: 2.5540
Epoch 18/100, Batch 60/274, Loss: 2.6757
Epoch 18/100, Batch 70/274, Loss: 2.5815
Epoch 18/100, Batch 80/274, Loss: 2.5589
Epoch 18/100, Batch 90/274, Loss: 2.5193
Epoch 18/100, Batch 100/274, Loss: 2.6599
Epoch 18/100, Batch 110/274, Loss: 2.5736
Epoch 18/100, Batch 120/274, Loss: 2.5733
Epoch 18/100, Batch 130/274, Loss: 2.5346
Epoch 18/100, Batch 140/274, Loss: 2.5582
Epoch 18/100, Batch 150/274, Loss: 2.5285
Epoch 18/100, Batch 160/274, Loss: 2.5611
Epoch 18/100, Batch 170/274, Loss: 2.6896
Epoch 18/100, Batch 180/274, Loss: 2.5769
Epoch 18/100, Batch 190/274, Loss: 2.6312
Epoch 18/100, Batch 200/274, Loss: 2.5495
Epoch 18/100, Batch 210/274, Loss: 2.5757
Epoch 18/100, Batch 220/274, Loss: 2.5484
Epoch 18/100, Batch 230/274, Loss: 2.6043
Epoch 18/100, Batch 240/274, Loss: 2.6264
Epoch 18/100, Batch 250/274, Loss: 2.5959
Epoch 18/100, Batch 260/274, Loss: 2.6024
Epoch 18/100, Batch 270/274, Loss: 2.6062
New best model with validation loss: 2.4648, perplexity: 11.76
Epoch 18/100, Loss: 2.5814, Perplexity: 13.22, Val Loss: 2.4648, Val Perplexity: 11.76, Time: 369.28s
Epoch 19/100, Batch 10/274, Loss: 2.6077
Epoch 19/100, Batch 20/274, Loss: 2.5685
Epoch 19/100, Batch 30/274, Loss: 2.5476
Epoch 19/100, Batch 40/274, Loss: 2.5537
Epoch 19/100, Batch 50/274, Loss: 2.5193
Epoch 19/100, Batch 60/274, Loss: 2.6004
Epoch 19/100, Batch 70/274, Loss: 2.5781
Epoch 19/100, Batch 80/274, Loss: 2.5282
Epoch 19/100, Batch 90/274, Loss: 2.4822
Epoch 19/100, Batch 100/274, Loss: 2.5823
Epoch 19/100, Batch 110/274, Loss: 2.5461
Epoch 19/100, Batch 120/274, Loss: 2.5395
Epoch 19/100, Batch 130/274, Loss: 2.5067
Epoch 19/100, Batch 140/274, Loss: 2.5224
Epoch 19/100, Batch 150/274, Loss: 2.4939
Epoch 19/100, Batch 160/274, Loss: 2.5364
Epoch 19/100, Batch 170/274, Loss: 2.5522
Epoch 19/100, Batch 180/274, Loss: 2.5707
Epoch 19/100, Batch 190/274, Loss: 2.5811
Epoch 19/100, Batch 200/274, Loss: 2.5732
Epoch 19/100, Batch 210/274, Loss: 2.5581
Epoch 19/100, Batch 220/274, Loss: 2.5321
Epoch 19/100, Batch 230/274, Loss: 2.5774
Epoch 19/100, Batch 240/274, Loss: 2.5914
Epoch 19/100, Batch 250/274, Loss: 2.5687
Epoch 19/100, Batch 260/274, Loss: 2.5801
Epoch 19/100, Batch 270/274, Loss: 2.5479
New best model with validation loss: 2.4344, perplexity: 11.41
Epoch 19/100, Loss: 2.5501, Perplexity: 12.81, Val Loss: 2.4344, Val Perplexity: 11.41, Time: 370.93s
Epoch 20/100, Batch 10/274, Loss: 2.6163
Epoch 20/100, Batch 20/274, Loss: 2.5381
Epoch 20/100, Batch 30/274, Loss: 2.5114
Epoch 20/100, Batch 40/274, Loss: 2.5268
Epoch 20/100, Batch 50/274, Loss: 2.5207
Epoch 20/100, Batch 60/274, Loss: 2.6046
Epoch 20/100, Batch 70/274, Loss: 2.5198
Epoch 20/100, Batch 80/274, Loss: 2.5171
Epoch 20/100, Batch 90/274, Loss: 2.4557
Epoch 20/100, Batch 100/274, Loss: 2.4850
Epoch 20/100, Batch 110/274, Loss: 2.5480
Epoch 20/100, Batch 120/274, Loss: 2.5193
Epoch 20/100, Batch 130/274, Loss: 2.4835
Epoch 20/100, Batch 140/274, Loss: 2.5080
Epoch 20/100, Batch 150/274, Loss: 2.4997
Epoch 20/100, Batch 160/274, Loss: 2.5076
Epoch 20/100, Batch 170/274, Loss: 2.5432
Epoch 20/100, Batch 180/274, Loss: 2.5279
Epoch 20/100, Batch 190/274, Loss: 2.6506
Epoch 20/100, Batch 200/274, Loss: 2.5104
Epoch 20/100, Batch 210/274, Loss: 2.5416
Epoch 20/100, Batch 220/274, Loss: 2.5092
Epoch 20/100, Batch 230/274, Loss: 2.5549
Epoch 20/100, Batch 240/274, Loss: 2.5610
Epoch 20/100, Batch 250/274, Loss: 2.5429
Epoch 20/100, Batch 260/274, Loss: 2.5597
Epoch 20/100, Batch 270/274, Loss: 2.5175
New best model with validation loss: 2.4093, perplexity: 11.13
Epoch 20/100, Loss: 2.5257, Perplexity: 12.50, Val Loss: 2.4093, Val Perplexity: 11.13, Time: 369.69s
Epoch 21/100, Batch 10/274, Loss: 2.5597
Epoch 21/100, Batch 20/274, Loss: 2.5145
Epoch 21/100, Batch 30/274, Loss: 2.5225
Epoch 21/100, Batch 40/274, Loss: 2.5036
Epoch 21/100, Batch 50/274, Loss: 2.4663
Epoch 21/100, Batch 60/274, Loss: 2.5430
Epoch 21/100, Batch 70/274, Loss: 2.5035
Epoch 21/100, Batch 80/274, Loss: 2.4795
Epoch 21/100, Batch 90/274, Loss: 2.4341
Epoch 21/100, Batch 100/274, Loss: 2.4633
Epoch 21/100, Batch 110/274, Loss: 2.4971
Epoch 21/100, Batch 120/274, Loss: 2.4924
Epoch 21/100, Batch 130/274, Loss: 2.4557
Epoch 21/100, Batch 140/274, Loss: 2.4909
Epoch 21/100, Batch 150/274, Loss: 2.4452
Epoch 21/100, Batch 160/274, Loss: 2.4908
Epoch 21/100, Batch 170/274, Loss: 2.5041
Epoch 21/100, Batch 180/274, Loss: 2.5451
Epoch 21/100, Batch 190/274, Loss: 2.5295
Epoch 21/100, Batch 200/274, Loss: 2.4712
Epoch 21/100, Batch 210/274, Loss: 2.5054
Epoch 21/100, Batch 220/274, Loss: 2.4711
Epoch 21/100, Batch 230/274, Loss: 2.6017
Epoch 21/100, Batch 240/274, Loss: 2.5506
Epoch 21/100, Batch 250/274, Loss: 2.5190
Epoch 21/100, Batch 260/274, Loss: 2.5621
Epoch 21/100, Batch 270/274, Loss: 2.5033
New best model with validation loss: 2.3922, perplexity: 10.94
Epoch 21/100, Loss: 2.5023, Perplexity: 12.21, Val Loss: 2.3922, Val Perplexity: 10.94, Time: 369.78s
Epoch 22/100, Batch 10/274, Loss: 2.5394
Epoch 22/100, Batch 20/274, Loss: 2.4977
Epoch 22/100, Batch 30/274, Loss: 2.4683
Epoch 22/100, Batch 40/274, Loss: 2.4831
Epoch 22/100, Batch 50/274, Loss: 2.4522
Epoch 22/100, Batch 60/274, Loss: 2.5190
Epoch 22/100, Batch 70/274, Loss: 2.4785
Epoch 22/100, Batch 80/274, Loss: 2.4576
Epoch 22/100, Batch 90/274, Loss: 2.4120
Epoch 22/100, Batch 100/274, Loss: 2.4423
Epoch 22/100, Batch 110/274, Loss: 2.4716
Epoch 22/100, Batch 120/274, Loss: 2.5266
Epoch 22/100, Batch 130/274, Loss: 2.4432
Epoch 22/100, Batch 140/274, Loss: 2.4764
Epoch 22/100, Batch 150/274, Loss: 2.4264
Epoch 22/100, Batch 160/274, Loss: 2.4789
Epoch 22/100, Batch 170/274, Loss: 2.5250
Epoch 22/100, Batch 180/274, Loss: 2.4986
Epoch 22/100, Batch 190/274, Loss: 2.5164
Epoch 22/100, Batch 200/274, Loss: 2.4864
Epoch 22/100, Batch 210/274, Loss: 2.4828
Epoch 22/100, Batch 220/274, Loss: 2.4558
Epoch 22/100, Batch 230/274, Loss: 2.5125
Epoch 22/100, Batch 240/274, Loss: 2.5190
Epoch 22/100, Batch 250/274, Loss: 2.6081
Epoch 22/100, Batch 260/274, Loss: 2.5095
Epoch 22/100, Batch 270/274, Loss: 2.5010
New best model with validation loss: 2.3742, perplexity: 10.74
Epoch 22/100, Loss: 2.4831, Perplexity: 11.98, Val Loss: 2.3742, Val Perplexity: 10.74, Time: 369.32s
Epoch 23/100, Batch 10/274, Loss: 2.5095
Epoch 23/100, Batch 20/274, Loss: 2.4713
Epoch 23/100, Batch 30/274, Loss: 2.4569
Epoch 23/100, Batch 40/274, Loss: 2.4616
Epoch 23/100, Batch 50/274, Loss: 2.4385
Epoch 23/100, Batch 60/274, Loss: 2.5016
Epoch 23/100, Batch 70/274, Loss: 2.5115
Epoch 23/100, Batch 80/274, Loss: 2.4409
Epoch 23/100, Batch 90/274, Loss: 2.4883
Epoch 23/100, Batch 100/274, Loss: 2.4218
Epoch 23/100, Batch 110/274, Loss: 2.4558
Epoch 23/100, Batch 120/274, Loss: 2.4581
Epoch 23/100, Batch 130/274, Loss: 2.4189
Epoch 23/100, Batch 140/274, Loss: 2.4407
Epoch 23/100, Batch 150/274, Loss: 2.4233
Epoch 23/100, Batch 160/274, Loss: 2.4574
Epoch 23/100, Batch 170/274, Loss: 2.4707
Epoch 23/100, Batch 180/274, Loss: 2.4712
Epoch 23/100, Batch 190/274, Loss: 2.5002
Epoch 23/100, Batch 200/274, Loss: 2.4395
Epoch 23/100, Batch 210/274, Loss: 2.4656
Epoch 23/100, Batch 220/274, Loss: 2.5296
Epoch 23/100, Batch 230/274, Loss: 2.4863
Epoch 23/100, Batch 240/274, Loss: 2.5017
Epoch 23/100, Batch 250/274, Loss: 2.5012
Epoch 23/100, Batch 260/274, Loss: 2.5270
Epoch 23/100, Batch 270/274, Loss: 2.5031
New best model with validation loss: 2.3630, perplexity: 10.62
Epoch 23/100, Loss: 2.4674, Perplexity: 11.79, Val Loss: 2.3630, Val Perplexity: 10.62, Time: 368.68s
Epoch 24/100, Batch 10/274, Loss: 2.5196
Epoch 24/100, Batch 20/274, Loss: 2.4583
Epoch 24/100, Batch 30/274, Loss: 2.4330
Epoch 24/100, Batch 40/274, Loss: 2.4409
Epoch 24/100, Batch 50/274, Loss: 2.4845
Epoch 24/100, Batch 60/274, Loss: 2.4833
Epoch 24/100, Batch 70/274, Loss: 2.4376
Epoch 24/100, Batch 80/274, Loss: 2.4562
Epoch 24/100, Batch 90/274, Loss: 2.3782
Epoch 24/100, Batch 100/274, Loss: 2.3993
Epoch 24/100, Batch 110/274, Loss: 2.4365
Epoch 24/100, Batch 120/274, Loss: 2.4480
Epoch 24/100, Batch 130/274, Loss: 2.4016
Epoch 24/100, Batch 140/274, Loss: 2.4449
Epoch 24/100, Batch 150/274, Loss: 2.3921
Epoch 24/100, Batch 160/274, Loss: 2.4737
Epoch 24/100, Batch 170/274, Loss: 2.4523
Epoch 24/100, Batch 180/274, Loss: 2.4514
Epoch 24/100, Batch 190/274, Loss: 2.4697
Epoch 24/100, Batch 200/274, Loss: 2.4232
Epoch 24/100, Batch 210/274, Loss: 2.4436
Epoch 24/100, Batch 220/274, Loss: 2.4226
Epoch 24/100, Batch 230/274, Loss: 2.4795
Epoch 24/100, Batch 240/274, Loss: 2.4797
Epoch 24/100, Batch 250/274, Loss: 2.4828
Epoch 24/100, Batch 260/274, Loss: 2.5208
Epoch 24/100, Batch 270/274, Loss: 2.4583
New best model with validation loss: 2.3420, perplexity: 10.40
Epoch 24/100, Loss: 2.4469, Perplexity: 11.55, Val Loss: 2.3420, Val Perplexity: 10.40, Time: 369.11s
Epoch 25/100, Batch 10/274, Loss: 2.4758
Epoch 25/100, Batch 20/274, Loss: 2.4361
Epoch 25/100, Batch 30/274, Loss: 2.4261
Epoch 25/100, Batch 40/274, Loss: 2.4264
Epoch 25/100, Batch 50/274, Loss: 2.4048
Epoch 25/100, Batch 60/274, Loss: 2.4771
Epoch 25/100, Batch 70/274, Loss: 2.4182
Epoch 25/100, Batch 80/274, Loss: 2.4019
Epoch 25/100, Batch 90/274, Loss: 2.3652
Epoch 25/100, Batch 100/274, Loss: 2.3975
Epoch 25/100, Batch 110/274, Loss: 2.4184
Epoch 25/100, Batch 120/274, Loss: 2.4542
Epoch 25/100, Batch 130/274, Loss: 2.3836
Epoch 25/100, Batch 140/274, Loss: 2.4098
Epoch 25/100, Batch 150/274, Loss: 2.3736
Epoch 25/100, Batch 160/274, Loss: 2.4111
Epoch 25/100, Batch 170/274, Loss: 2.4392
Epoch 25/100, Batch 180/274, Loss: 2.5388
Epoch 25/100, Batch 190/274, Loss: 2.4622
Epoch 25/100, Batch 200/274, Loss: 2.4069
Epoch 25/100, Batch 210/274, Loss: 2.4441
Epoch 25/100, Batch 220/274, Loss: 2.4317
Epoch 25/100, Batch 230/274, Loss: 2.4538
Epoch 25/100, Batch 240/274, Loss: 2.4644
Epoch 25/100, Batch 250/274, Loss: 2.4476
Epoch 25/100, Batch 260/274, Loss: 2.4717
Epoch 25/100, Batch 270/274, Loss: 2.4207
New best model with validation loss: 2.3304, perplexity: 10.28
Epoch 25/100, Loss: 2.4306, Perplexity: 11.37, Val Loss: 2.3304, Val Perplexity: 10.28, Time: 368.35s
Epoch 26/100, Batch 10/274, Loss: 2.4658
Epoch 26/100, Batch 20/274, Loss: 2.4236
Epoch 26/100, Batch 30/274, Loss: 2.3998
Epoch 26/100, Batch 40/274, Loss: 2.4148
Epoch 26/100, Batch 50/274, Loss: 2.3945
Epoch 26/100, Batch 60/274, Loss: 2.4471
Epoch 26/100, Batch 70/274, Loss: 2.4092
Epoch 26/100, Batch 80/274, Loss: 2.4380
Epoch 26/100, Batch 90/274, Loss: 2.3548
Epoch 26/100, Batch 100/274, Loss: 2.3679
Epoch 26/100, Batch 110/274, Loss: 2.4085
Epoch 26/100, Batch 120/274, Loss: 2.4298
Epoch 26/100, Batch 130/274, Loss: 2.3719
Epoch 26/100, Batch 140/274, Loss: 2.4081
Epoch 26/100, Batch 150/274, Loss: 2.3690
Epoch 26/100, Batch 160/274, Loss: 2.4299
Epoch 26/100, Batch 170/274, Loss: 2.4266
Epoch 26/100, Batch 180/274, Loss: 2.4483
Epoch 26/100, Batch 190/274, Loss: 2.4485
Epoch 26/100, Batch 200/274, Loss: 2.3936
Epoch 26/100, Batch 210/274, Loss: 2.4249
Epoch 26/100, Batch 220/274, Loss: 2.3926
Epoch 26/100, Batch 230/274, Loss: 2.4409
Epoch 26/100, Batch 240/274, Loss: 2.4494
Epoch 26/100, Batch 250/274, Loss: 2.4394
Epoch 26/100, Batch 260/274, Loss: 2.4658
Epoch 26/100, Batch 270/274, Loss: 2.4153
New best model with validation loss: 2.3219, perplexity: 10.19
Epoch 26/100, Loss: 2.4192, Perplexity: 11.24, Val Loss: 2.3219, Val Perplexity: 10.19, Time: 367.57s
Epoch 27/100, Batch 10/274, Loss: 2.4577
Epoch 27/100, Batch 20/274, Loss: 2.4402
Epoch 27/100, Batch 30/274, Loss: 2.4006
Epoch 27/100, Batch 40/274, Loss: 2.4000
Epoch 27/100, Batch 50/274, Loss: 2.3752
Epoch 27/100, Batch 60/274, Loss: 2.4481
Epoch 27/100, Batch 70/274, Loss: 2.3942
Epoch 27/100, Batch 80/274, Loss: 2.3772
Epoch 27/100, Batch 90/274, Loss: 2.3384
Epoch 27/100, Batch 100/274, Loss: 2.3563
Epoch 27/100, Batch 110/274, Loss: 2.3957
Epoch 27/100, Batch 120/274, Loss: 2.4005
Epoch 27/100, Batch 130/274, Loss: 2.3634
Epoch 27/100, Batch 140/274, Loss: 2.3825
Epoch 27/100, Batch 150/274, Loss: 2.3458
Epoch 27/100, Batch 160/274, Loss: 2.3815
Epoch 27/100, Batch 170/274, Loss: 2.4144
Epoch 27/100, Batch 180/274, Loss: 2.4180
Epoch 27/100, Batch 190/274, Loss: 2.4272
Epoch 27/100, Batch 200/274, Loss: 2.3818
Epoch 27/100, Batch 210/274, Loss: 2.4080
Epoch 27/100, Batch 220/274, Loss: 2.4090
Epoch 27/100, Batch 230/274, Loss: 2.4284
Epoch 27/100, Batch 240/274, Loss: 2.4304
Epoch 27/100, Batch 250/274, Loss: 2.4335
Epoch 27/100, Batch 260/274, Loss: 2.4378
Epoch 27/100, Batch 270/274, Loss: 2.3922
New best model with validation loss: 2.3072, perplexity: 10.05
Epoch 27/100, Loss: 2.4041, Perplexity: 11.07, Val Loss: 2.3072, Val Perplexity: 10.05, Time: 368.02s
Epoch 28/100, Batch 10/274, Loss: 2.4740
Epoch 28/100, Batch 20/274, Loss: 2.3951
Epoch 28/100, Batch 30/274, Loss: 2.3730
Epoch 28/100, Batch 40/274, Loss: 2.3953
Epoch 28/100, Batch 50/274, Loss: 2.3923
Epoch 28/100, Batch 60/274, Loss: 2.4241
Epoch 28/100, Batch 70/274, Loss: 2.3831
Epoch 28/100, Batch 80/274, Loss: 2.3948
Epoch 28/100, Batch 90/274, Loss: 2.3193
Epoch 28/100, Batch 100/274, Loss: 2.3488
Epoch 28/100, Batch 110/274, Loss: 2.3792
Epoch 28/100, Batch 120/274, Loss: 2.3872
Epoch 28/100, Batch 130/274, Loss: 2.3694
Epoch 28/100, Batch 140/274, Loss: 2.3739
Epoch 28/100, Batch 150/274, Loss: 2.3321
Epoch 28/100, Batch 160/274, Loss: 2.3660
Epoch 28/100, Batch 170/274, Loss: 2.4427
Epoch 28/100, Batch 180/274, Loss: 2.4136
Epoch 28/100, Batch 190/274, Loss: 2.4246
Epoch 28/100, Batch 200/274, Loss: 2.3630
Epoch 28/100, Batch 210/274, Loss: 2.3999
Epoch 28/100, Batch 220/274, Loss: 2.3782
Epoch 28/100, Batch 230/274, Loss: 2.4248
Epoch 28/100, Batch 240/274, Loss: 2.4249
Epoch 28/100, Batch 250/274, Loss: 2.4191
Epoch 28/100, Batch 260/274, Loss: 2.4391
Epoch 28/100, Batch 270/274, Loss: 2.3849
New best model with validation loss: 2.3040, perplexity: 10.01
Epoch 28/100, Loss: 2.3893, Perplexity: 10.91, Val Loss: 2.3040, Val Perplexity: 10.01, Time: 368.94s
Epoch 29/100, Batch 10/274, Loss: 2.4252
Epoch 29/100, Batch 20/274, Loss: 2.3919
Epoch 29/100, Batch 30/274, Loss: 2.3599
Epoch 29/100, Batch 40/274, Loss: 2.3736
Epoch 29/100, Batch 50/274, Loss: 2.3473
Epoch 29/100, Batch 60/274, Loss: 2.4129
Epoch 29/100, Batch 70/274, Loss: 2.3691
Epoch 29/100, Batch 80/274, Loss: 2.3496
Epoch 29/100, Batch 90/274, Loss: 2.3168
Epoch 29/100, Batch 100/274, Loss: 2.3266
Epoch 29/100, Batch 110/274, Loss: 2.3905
Epoch 29/100, Batch 120/274, Loss: 2.3758
Epoch 29/100, Batch 130/274, Loss: 2.3397
Epoch 29/100, Batch 140/274, Loss: 2.3579
Epoch 29/100, Batch 150/274, Loss: 2.3572
Epoch 29/100, Batch 160/274, Loss: 2.3533
Epoch 29/100, Batch 170/274, Loss: 2.3857
Epoch 29/100, Batch 180/274, Loss: 2.3969
Epoch 29/100, Batch 190/274, Loss: 2.4032
Epoch 29/100, Batch 200/274, Loss: 2.3531
Epoch 29/100, Batch 210/274, Loss: 2.3859
Epoch 29/100, Batch 220/274, Loss: 2.3496
Epoch 29/100, Batch 230/274, Loss: 2.4181
Epoch 29/100, Batch 240/274, Loss: 2.4106
Epoch 29/100, Batch 250/274, Loss: 2.3943
Epoch 29/100, Batch 260/274, Loss: 2.4163
Epoch 29/100, Batch 270/274, Loss: 2.3720
New best model with validation loss: 2.2922, perplexity: 9.90
Epoch 29/100, Loss: 2.3772, Perplexity: 10.77, Val Loss: 2.2922, Val Perplexity: 9.90, Time: 368.73s
Epoch 30/100, Batch 10/274, Loss: 2.4227
Epoch 30/100, Batch 20/274, Loss: 2.3714
Epoch 30/100, Batch 30/274, Loss: 2.3563
Epoch 30/100, Batch 40/274, Loss: 2.3585
Epoch 30/100, Batch 50/274, Loss: 2.3351
Epoch 30/100, Batch 60/274, Loss: 2.4029
Epoch 30/100, Batch 70/274, Loss: 2.3668
Epoch 30/100, Batch 80/274, Loss: 2.3927
Epoch 30/100, Batch 90/274, Loss: 2.2976
Epoch 30/100, Batch 100/274, Loss: 2.3512
Epoch 30/100, Batch 110/274, Loss: 2.3579
Epoch 30/100, Batch 120/274, Loss: 2.3679
Epoch 30/100, Batch 130/274, Loss: 2.3196
Epoch 30/100, Batch 140/274, Loss: 2.3367
Epoch 30/100, Batch 150/274, Loss: 2.3220
Epoch 30/100, Batch 160/274, Loss: 2.3398
Epoch 30/100, Batch 170/274, Loss: 2.3659
Epoch 30/100, Batch 180/274, Loss: 2.3857
Epoch 30/100, Batch 190/274, Loss: 2.3966
Epoch 30/100, Batch 200/274, Loss: 2.3381
Epoch 30/100, Batch 210/274, Loss: 2.3710
Epoch 30/100, Batch 220/274, Loss: 2.3381
Epoch 30/100, Batch 230/274, Loss: 2.3981
Epoch 30/100, Batch 240/274, Loss: 2.3971
Epoch 30/100, Batch 250/274, Loss: 2.4326
Epoch 30/100, Batch 260/274, Loss: 2.4040
Epoch 30/100, Batch 270/274, Loss: 2.3719
New best model with validation loss: 2.2877, perplexity: 9.85
Epoch 30/100, Loss: 2.3688, Perplexity: 10.68, Val Loss: 2.2877, Val Perplexity: 9.85, Time: 367.34s
Epoch 31/100, Batch 10/274, Loss: 2.4083
Epoch 31/100, Batch 20/274, Loss: 2.3644
Epoch 31/100, Batch 30/274, Loss: 2.3435
Epoch 31/100, Batch 40/274, Loss: 2.3495
Epoch 31/100, Batch 50/274, Loss: 2.3217
Epoch 31/100, Batch 60/274, Loss: 2.4203
Epoch 31/100, Batch 70/274, Loss: 2.3506
Epoch 31/100, Batch 80/274, Loss: 2.3288
Epoch 31/100, Batch 90/274, Loss: 2.3472
Epoch 31/100, Batch 100/274, Loss: 2.3052
Epoch 31/100, Batch 110/274, Loss: 2.3636
Epoch 31/100, Batch 120/274, Loss: 2.3460
Epoch 31/100, Batch 130/274, Loss: 2.4622
Epoch 31/100, Batch 140/274, Loss: 2.3328
Epoch 31/100, Batch 150/274, Loss: 2.3136
Epoch 31/100, Batch 160/274, Loss: 2.3320
Epoch 31/100, Batch 170/274, Loss: 2.3640
Epoch 31/100, Batch 180/274, Loss: 2.3724
Epoch 31/100, Batch 190/274, Loss: 2.3913
Epoch 31/100, Batch 200/274, Loss: 2.3272
Epoch 31/100, Batch 210/274, Loss: 2.3616
Epoch 31/100, Batch 220/274, Loss: 2.3270
Epoch 31/100, Batch 230/274, Loss: 2.3791
Epoch 31/100, Batch 240/274, Loss: 2.3860
Epoch 31/100, Batch 250/274, Loss: 2.3830
Epoch 31/100, Batch 260/274, Loss: 2.4045
Epoch 31/100, Batch 270/274, Loss: 2.3558
New best model with validation loss: 2.2739, perplexity: 9.72
Epoch 31/100, Loss: 2.3572, Perplexity: 10.56, Val Loss: 2.2739, Val Perplexity: 9.72, Time: 368.59s
Epoch 32/100, Batch 10/274, Loss: 2.4063
Epoch 32/100, Batch 20/274, Loss: 2.4151
Epoch 32/100, Batch 30/274, Loss: 2.3319
Epoch 32/100, Batch 40/274, Loss: 2.3356
Epoch 32/100, Batch 50/274, Loss: 2.3441
Epoch 32/100, Batch 60/274, Loss: 2.3760
Epoch 32/100, Batch 70/274, Loss: 2.3492
Epoch 32/100, Batch 80/274, Loss: 2.3554
Epoch 32/100, Batch 90/274, Loss: 2.2757
Epoch 32/100, Batch 100/274, Loss: 2.2982
Epoch 32/100, Batch 110/274, Loss: 2.3482
Epoch 32/100, Batch 120/274, Loss: 2.3412
Epoch 32/100, Batch 130/274, Loss: 2.3835
Epoch 32/100, Batch 140/274, Loss: 2.3267
Epoch 32/100, Batch 150/274, Loss: 2.2994
Epoch 32/100, Batch 160/274, Loss: 2.3172
Epoch 32/100, Batch 170/274, Loss: 2.3712
Epoch 32/100, Batch 180/274, Loss: 2.3605
Epoch 32/100, Batch 190/274, Loss: 2.3698
Epoch 32/100, Batch 200/274, Loss: 2.3260
Epoch 32/100, Batch 210/274, Loss: 2.4010
Epoch 32/100, Batch 220/274, Loss: 2.3218
Epoch 32/100, Batch 230/274, Loss: 2.3744
Epoch 32/100, Batch 240/274, Loss: 2.3810
Epoch 32/100, Batch 250/274, Loss: 2.3665
Epoch 32/100, Batch 260/274, Loss: 2.3912
Epoch 32/100, Batch 270/274, Loss: 2.3380
New best model with validation loss: 2.2645, perplexity: 9.63
Epoch 32/100, Loss: 2.3493, Perplexity: 10.48, Val Loss: 2.2645, Val Perplexity: 9.63, Time: 367.47s
Epoch 33/100, Batch 10/274, Loss: 2.3968
Epoch 33/100, Batch 20/274, Loss: 2.3528
Epoch 33/100, Batch 30/274, Loss: 2.3269
Epoch 33/100, Batch 40/274, Loss: 2.3360
Epoch 33/100, Batch 50/274, Loss: 2.3047
Epoch 33/100, Batch 60/274, Loss: 2.3822
Epoch 33/100, Batch 70/274, Loss: 2.3258
Epoch 33/100, Batch 80/274, Loss: 2.3124
Epoch 33/100, Batch 90/274, Loss: 2.2742
Epoch 33/100, Batch 100/274, Loss: 2.2937
Epoch 33/100, Batch 110/274, Loss: 2.3696
Epoch 33/100, Batch 120/274, Loss: 2.3305
Epoch 33/100, Batch 130/274, Loss: 2.2967
Epoch 33/100, Batch 140/274, Loss: 2.3210
Epoch 33/100, Batch 150/274, Loss: 2.2869
Epoch 33/100, Batch 160/274, Loss: 2.3189
Epoch 33/100, Batch 170/274, Loss: 2.3411
Epoch 33/100, Batch 180/274, Loss: 2.3556
Epoch 33/100, Batch 190/274, Loss: 2.3714
Epoch 33/100, Batch 200/274, Loss: 2.3045
Epoch 33/100, Batch 210/274, Loss: 2.3533
Epoch 33/100, Batch 220/274, Loss: 2.3414
Epoch 33/100, Batch 230/274, Loss: 2.3626
Epoch 33/100, Batch 240/274, Loss: 2.3655
Epoch 33/100, Batch 250/274, Loss: 2.3560
Epoch 33/100, Batch 260/274, Loss: 2.3809
Epoch 33/100, Batch 270/274, Loss: 2.3414
New best model with validation loss: 2.2584, perplexity: 9.57
Epoch 33/100, Loss: 2.3376, Perplexity: 10.36, Val Loss: 2.2584, Val Perplexity: 9.57, Time: 369.17s
Epoch 34/100, Batch 10/274, Loss: 2.3778
Epoch 34/100, Batch 20/274, Loss: 2.3440
Epoch 34/100, Batch 30/274, Loss: 2.3133
Epoch 34/100, Batch 40/274, Loss: 2.3163
Epoch 34/100, Batch 50/274, Loss: 2.3061
Epoch 34/100, Batch 60/274, Loss: 2.3560
Epoch 34/100, Batch 70/274, Loss: 2.3156
Epoch 34/100, Batch 80/274, Loss: 2.2995
Epoch 34/100, Batch 90/274, Loss: 2.2554
Epoch 34/100, Batch 100/274, Loss: 2.2880
Epoch 34/100, Batch 110/274, Loss: 2.3231
Epoch 34/100, Batch 120/274, Loss: 2.3318
Epoch 34/100, Batch 130/274, Loss: 2.2868
Epoch 34/100, Batch 140/274, Loss: 2.3213
Epoch 34/100, Batch 150/274, Loss: 2.2798
Epoch 34/100, Batch 160/274, Loss: 2.2989
Epoch 34/100, Batch 170/274, Loss: 2.3854
Epoch 34/100, Batch 180/274, Loss: 2.3528
Epoch 34/100, Batch 190/274, Loss: 2.3455
Epoch 34/100, Batch 200/274, Loss: 2.3033
Epoch 34/100, Batch 210/274, Loss: 2.3320
Epoch 34/100, Batch 220/274, Loss: 2.2997
Epoch 34/100, Batch 230/274, Loss: 2.3536
Epoch 34/100, Batch 240/274, Loss: 2.3641
Epoch 34/100, Batch 250/274, Loss: 2.3579
Epoch 34/100, Batch 260/274, Loss: 2.3640
Epoch 34/100, Batch 270/274, Loss: 2.3338
New best model with validation loss: 2.2518, perplexity: 9.51
Epoch 34/100, Loss: 2.3290, Perplexity: 10.27, Val Loss: 2.2518, Val Perplexity: 9.51, Time: 368.16s
Epoch 35/100, Batch 10/274, Loss: 2.3720
Epoch 35/100, Batch 20/274, Loss: 2.3258
Epoch 35/100, Batch 30/274, Loss: 2.3128
Epoch 35/100, Batch 40/274, Loss: 2.3100
Epoch 35/100, Batch 50/274, Loss: 2.2900
Epoch 35/100, Batch 60/274, Loss: 2.3907
Epoch 35/100, Batch 70/274, Loss: 2.3665
Epoch 35/100, Batch 80/274, Loss: 2.2889
Epoch 35/100, Batch 90/274, Loss: 2.2575
Epoch 35/100, Batch 100/274, Loss: 2.2793
Epoch 35/100, Batch 110/274, Loss: 2.3144
Epoch 35/100, Batch 120/274, Loss: 2.3144
Epoch 35/100, Batch 130/274, Loss: 2.2783
Epoch 35/100, Batch 140/274, Loss: 2.2960
Epoch 35/100, Batch 150/274, Loss: 2.2643
Epoch 35/100, Batch 160/274, Loss: 2.3583
Epoch 35/100, Batch 170/274, Loss: 2.3298
Epoch 35/100, Batch 180/274, Loss: 2.3372
Epoch 35/100, Batch 190/274, Loss: 2.3469
Epoch 35/100, Batch 200/274, Loss: 2.3431
Epoch 35/100, Batch 210/274, Loss: 2.3538
Epoch 35/100, Batch 220/274, Loss: 2.2887
Epoch 35/100, Batch 230/274, Loss: 2.3854
Epoch 35/100, Batch 240/274, Loss: 2.3516
Epoch 35/100, Batch 250/274, Loss: 2.3424
Epoch 35/100, Batch 260/274, Loss: 2.3586
Epoch 35/100, Batch 270/274, Loss: 2.3210
New best model with validation loss: 2.2435, perplexity: 9.43
Epoch 35/100, Loss: 2.3194, Perplexity: 10.17, Val Loss: 2.2435, Val Perplexity: 9.43, Time: 368.92s
Epoch 36/100, Batch 10/274, Loss: 2.3625
Epoch 36/100, Batch 20/274, Loss: 2.3252
Epoch 36/100, Batch 30/274, Loss: 2.3067
Epoch 36/100, Batch 40/274, Loss: 2.3094
Epoch 36/100, Batch 50/274, Loss: 2.3037
Epoch 36/100, Batch 60/274, Loss: 2.3457
Epoch 36/100, Batch 70/274, Loss: 2.3107
Epoch 36/100, Batch 80/274, Loss: 2.2812
Epoch 36/100, Batch 90/274, Loss: 2.2454
Epoch 36/100, Batch 100/274, Loss: 2.2664
Epoch 36/100, Batch 110/274, Loss: 2.2986
Epoch 36/100, Batch 120/274, Loss: 2.3018
Epoch 36/100, Batch 130/274, Loss: 2.2679
Epoch 36/100, Batch 140/274, Loss: 2.2903
Epoch 36/100, Batch 150/274, Loss: 2.2618
Epoch 36/100, Batch 160/274, Loss: 2.2847
Epoch 36/100, Batch 170/274, Loss: 2.3129
Epoch 36/100, Batch 180/274, Loss: 2.3298
Epoch 36/100, Batch 190/274, Loss: 2.3839
Epoch 36/100, Batch 200/274, Loss: 2.2829
Epoch 36/100, Batch 210/274, Loss: 2.3169
Epoch 36/100, Batch 220/274, Loss: 2.2810
Epoch 36/100, Batch 230/274, Loss: 2.3372
Epoch 36/100, Batch 240/274, Loss: 2.3588
Epoch 36/100, Batch 250/274, Loss: 2.3592
Epoch 36/100, Batch 260/274, Loss: 2.3446
Epoch 36/100, Batch 270/274, Loss: 2.3130
New best model with validation loss: 2.2380, perplexity: 9.37
Epoch 36/100, Loss: 2.3120, Perplexity: 10.09, Val Loss: 2.2380, Val Perplexity: 9.37, Time: 369.50s
Epoch 37/100, Batch 10/274, Loss: 2.3505
Epoch 37/100, Batch 20/274, Loss: 2.3087
Epoch 37/100, Batch 30/274, Loss: 2.3174
Epoch 37/100, Batch 40/274, Loss: 2.2928
Epoch 37/100, Batch 50/274, Loss: 2.3240
Epoch 37/100, Batch 60/274, Loss: 2.3365
Epoch 37/100, Batch 70/274, Loss: 2.2911
Epoch 37/100, Batch 80/274, Loss: 2.2803
Epoch 37/100, Batch 90/274, Loss: 2.2452
Epoch 37/100, Batch 100/274, Loss: 2.2870
Epoch 37/100, Batch 110/274, Loss: 2.2971
Epoch 37/100, Batch 120/274, Loss: 2.3502
Epoch 37/100, Batch 130/274, Loss: 2.2843
Epoch 37/100, Batch 140/274, Loss: 2.2792
Epoch 37/100, Batch 150/274, Loss: 2.2544
Epoch 37/100, Batch 160/274, Loss: 2.2948
Epoch 37/100, Batch 170/274, Loss: 2.3494
Epoch 37/100, Batch 180/274, Loss: 2.3268
Epoch 37/100, Batch 190/274, Loss: 2.3324
Epoch 37/100, Batch 200/274, Loss: 2.2828
Epoch 37/100, Batch 210/274, Loss: 2.3145
Epoch 37/100, Batch 220/274, Loss: 2.2705
Epoch 37/100, Batch 230/274, Loss: 2.3360
Epoch 37/100, Batch 240/274, Loss: 2.3372
Epoch 37/100, Batch 250/274, Loss: 2.3467
Epoch 37/100, Batch 260/274, Loss: 2.3478
Epoch 37/100, Batch 270/274, Loss: 2.3071
Epoch 37/100, Loss: 2.3045, Perplexity: 10.02, Val Loss: 2.2438, Val Perplexity: 9.43, Time: 369.06s
Epoch 38/100, Batch 10/274, Loss: 2.3566
Epoch 38/100, Batch 20/274, Loss: 2.3066
Epoch 38/100, Batch 30/274, Loss: 2.3560
Epoch 38/100, Batch 40/274, Loss: 2.2891
Epoch 38/100, Batch 50/274, Loss: 2.2656
Epoch 38/100, Batch 60/274, Loss: 2.4078
Epoch 38/100, Batch 70/274, Loss: 2.2824
Epoch 38/100, Batch 80/274, Loss: 2.2681
Epoch 38/100, Batch 90/274, Loss: 2.2311
Epoch 38/100, Batch 100/274, Loss: 2.2473
Epoch 38/100, Batch 110/274, Loss: 2.3218
Epoch 38/100, Batch 120/274, Loss: 2.2873
Epoch 38/100, Batch 130/274, Loss: 2.2531
Epoch 38/100, Batch 140/274, Loss: 2.2774
Epoch 38/100, Batch 150/274, Loss: 2.2503
Epoch 38/100, Batch 160/274, Loss: 2.2692
Epoch 38/100, Batch 170/274, Loss: 2.3118
Epoch 38/100, Batch 180/274, Loss: 2.3510
Epoch 38/100, Batch 190/274, Loss: 2.3267
Epoch 38/100, Batch 200/274, Loss: 2.2755
Epoch 38/100, Batch 210/274, Loss: 2.3129
Epoch 38/100, Batch 220/274, Loss: 2.2768
Epoch 38/100, Batch 230/274, Loss: 2.4124
Epoch 38/100, Batch 240/274, Loss: 2.3471
Epoch 38/100, Batch 250/274, Loss: 2.3262
Epoch 38/100, Batch 260/274, Loss: 2.3673
Epoch 38/100, Batch 270/274, Loss: 2.2914
New best model with validation loss: 2.2275, perplexity: 9.28
Epoch 38/100, Loss: 2.2981, Perplexity: 9.95, Val Loss: 2.2275, Val Perplexity: 9.28, Time: 369.09s
Epoch 39/100, Batch 10/274, Loss: 2.3422
Epoch 39/100, Batch 20/274, Loss: 2.3342
Epoch 39/100, Batch 30/274, Loss: 2.2752
Epoch 39/100, Batch 40/274, Loss: 2.2809
Epoch 39/100, Batch 50/274, Loss: 2.2680
Epoch 39/100, Batch 60/274, Loss: 2.3330
Epoch 39/100, Batch 70/274, Loss: 2.2743
Epoch 39/100, Batch 80/274, Loss: 2.2612
Epoch 39/100, Batch 90/274, Loss: 2.2378
Epoch 39/100, Batch 100/274, Loss: 2.3191
Epoch 39/100, Batch 110/274, Loss: 2.2850
Epoch 39/100, Batch 120/274, Loss: 2.3088
Epoch 39/100, Batch 130/274, Loss: 2.2650
Epoch 39/100, Batch 140/274, Loss: 2.2744
Epoch 39/100, Batch 150/274, Loss: 2.2423
Epoch 39/100, Batch 160/274, Loss: 2.2795
Epoch 39/100, Batch 170/274, Loss: 2.3040
Epoch 39/100, Batch 180/274, Loss: 2.3169
Epoch 39/100, Batch 190/274, Loss: 2.3871
Epoch 39/100, Batch 200/274, Loss: 2.2635
Epoch 39/100, Batch 210/274, Loss: 2.3025
Epoch 39/100, Batch 220/274, Loss: 2.2539
Epoch 39/100, Batch 230/274, Loss: 2.3486
Epoch 39/100, Batch 240/274, Loss: 2.3182
Epoch 39/100, Batch 250/274, Loss: 2.3136
Epoch 39/100, Batch 260/274, Loss: 2.3285
Epoch 39/100, Batch 270/274, Loss: 2.2868
New best model with validation loss: 2.2234, perplexity: 9.24
Epoch 39/100, Loss: 2.2927, Perplexity: 9.90, Val Loss: 2.2234, Val Perplexity: 9.24, Time: 368.18s
Epoch 40/100, Batch 10/274, Loss: 2.3246
Epoch 40/100, Batch 20/274, Loss: 2.3514
Epoch 40/100, Batch 30/274, Loss: 2.2850
Epoch 40/100, Batch 40/274, Loss: 2.2839
Epoch 40/100, Batch 50/274, Loss: 2.2890
Epoch 40/100, Batch 60/274, Loss: 2.3154
Epoch 40/100, Batch 70/274, Loss: 2.2620
Epoch 40/100, Batch 80/274, Loss: 2.2472
Epoch 40/100, Batch 90/274, Loss: 2.2174
Epoch 40/100, Batch 100/274, Loss: 2.2344
Epoch 40/100, Batch 110/274, Loss: 2.2821
Epoch 40/100, Batch 120/274, Loss: 2.2753
Epoch 40/100, Batch 130/274, Loss: 2.2403
Epoch 40/100, Batch 140/274, Loss: 2.2572
Epoch 40/100, Batch 150/274, Loss: 2.2360
Epoch 40/100, Batch 160/274, Loss: 2.2555
Epoch 40/100, Batch 170/274, Loss: 2.2888
Epoch 40/100, Batch 180/274, Loss: 2.3075
Epoch 40/100, Batch 190/274, Loss: 2.3162
Epoch 40/100, Batch 200/274, Loss: 2.2651
Epoch 40/100, Batch 210/274, Loss: 2.2948
Epoch 40/100, Batch 220/274, Loss: 2.2530
Epoch 40/100, Batch 230/274, Loss: 2.3090
Epoch 40/100, Batch 240/274, Loss: 2.3215
Epoch 40/100, Batch 250/274, Loss: 2.3021
Epoch 40/100, Batch 260/274, Loss: 2.3993
Epoch 40/100, Batch 270/274, Loss: 2.3651
New best model with validation loss: 2.2213, perplexity: 9.22
Epoch 40/100, Loss: 2.2843, Perplexity: 9.82, Val Loss: 2.2213, Val Perplexity: 9.22, Time: 368.30s
Epoch 41/100, Batch 10/274, Loss: 2.3304
Epoch 41/100, Batch 20/274, Loss: 2.2910
Epoch 41/100, Batch 30/274, Loss: 2.2581
Epoch 41/100, Batch 40/274, Loss: 2.2665
Epoch 41/100, Batch 50/274, Loss: 2.2479
Epoch 41/100, Batch 60/274, Loss: 2.3135
Epoch 41/100, Batch 70/274, Loss: 2.3264
Epoch 41/100, Batch 80/274, Loss: 2.2464
Epoch 41/100, Batch 90/274, Loss: 2.2127
Epoch 41/100, Batch 100/274, Loss: 2.2303
Epoch 41/100, Batch 110/274, Loss: 2.2725
Epoch 41/100, Batch 120/274, Loss: 2.2654
Epoch 41/100, Batch 130/274, Loss: 2.2360
Epoch 41/100, Batch 140/274, Loss: 2.2526
Epoch 41/100, Batch 150/274, Loss: 2.2333
Epoch 41/100, Batch 160/274, Loss: 2.2518
Epoch 41/100, Batch 170/274, Loss: 2.2975
Epoch 41/100, Batch 180/274, Loss: 2.2946
Epoch 41/100, Batch 190/274, Loss: 2.3338
Epoch 41/100, Batch 200/274, Loss: 2.2526
Epoch 41/100, Batch 210/274, Loss: 2.2975
Epoch 41/100, Batch 220/274, Loss: 2.2432
Epoch 41/100, Batch 230/274, Loss: 2.3128
Epoch 41/100, Batch 240/274, Loss: 2.3115
Epoch 41/100, Batch 250/274, Loss: 2.3030
Epoch 41/100, Batch 260/274, Loss: 2.3135
Epoch 41/100, Batch 270/274, Loss: 2.2871
New best model with validation loss: 2.2131, perplexity: 9.14
Epoch 41/100, Loss: 2.2777, Perplexity: 9.75, Val Loss: 2.2131, Val Perplexity: 9.14, Time: 368.19s
Epoch 42/100, Batch 10/274, Loss: 2.3232
Epoch 42/100, Batch 20/274, Loss: 2.2800
Epoch 42/100, Batch 30/274, Loss: 2.2771
Epoch 42/100, Batch 40/274, Loss: 2.2640
Epoch 42/100, Batch 50/274, Loss: 2.2679
Epoch 42/100, Batch 60/274, Loss: 2.2957
Epoch 42/100, Batch 70/274, Loss: 2.2619
Epoch 42/100, Batch 80/274, Loss: 2.2374
Epoch 42/100, Batch 90/274, Loss: 2.2055
Epoch 42/100, Batch 100/274, Loss: 2.2270
Epoch 42/100, Batch 110/274, Loss: 2.2908
Epoch 42/100, Batch 120/274, Loss: 2.2585
Epoch 42/100, Batch 130/274, Loss: 2.3068
Epoch 42/100, Batch 140/274, Loss: 2.2517
Epoch 42/100, Batch 150/274, Loss: 2.2284
Epoch 42/100, Batch 160/274, Loss: 2.2431
Epoch 42/100, Batch 170/274, Loss: 2.2903
Epoch 42/100, Batch 180/274, Loss: 2.2939
Epoch 42/100, Batch 190/274, Loss: 2.3199
Epoch 42/100, Batch 200/274, Loss: 2.2459
Epoch 42/100, Batch 210/274, Loss: 2.2854
Epoch 42/100, Batch 220/274, Loss: 2.2371
Epoch 42/100, Batch 230/274, Loss: 2.2973
Epoch 42/100, Batch 240/274, Loss: 2.2982
Epoch 42/100, Batch 250/274, Loss: 2.2967
Epoch 42/100, Batch 260/274, Loss: 2.3057
Epoch 42/100, Batch 270/274, Loss: 2.2754
New best model with validation loss: 2.2123, perplexity: 9.14
Epoch 42/100, Loss: 2.2732, Perplexity: 9.71, Val Loss: 2.2123, Val Perplexity: 9.14, Time: 367.95s
Epoch 43/100, Batch 10/274, Loss: 2.3169
Epoch 43/100, Batch 20/274, Loss: 2.3362
Epoch 43/100, Batch 30/274, Loss: 2.2757
Epoch 43/100, Batch 40/274, Loss: 2.2473
Epoch 43/100, Batch 50/274, Loss: 2.2335
Epoch 43/100, Batch 60/274, Loss: 2.3251
Epoch 43/100, Batch 70/274, Loss: 2.2512
Epoch 43/100, Batch 80/274, Loss: 2.2311
Epoch 43/100, Batch 90/274, Loss: 2.1980
Epoch 43/100, Batch 100/274, Loss: 2.2311
Epoch 43/100, Batch 110/274, Loss: 2.2592
Epoch 43/100, Batch 120/274, Loss: 2.2893
Epoch 43/100, Batch 130/274, Loss: 2.2228
Epoch 43/100, Batch 140/274, Loss: 2.2357
Epoch 43/100, Batch 150/274, Loss: 2.2170
Epoch 43/100, Batch 160/274, Loss: 2.2339
Epoch 43/100, Batch 170/274, Loss: 2.2689
Epoch 43/100, Batch 180/274, Loss: 2.2991
Epoch 43/100, Batch 190/274, Loss: 2.3016
Epoch 43/100, Batch 200/274, Loss: 2.2378
Epoch 43/100, Batch 210/274, Loss: 2.3002
Epoch 43/100, Batch 220/274, Loss: 2.2365
Epoch 43/100, Batch 230/274, Loss: 2.2974
Epoch 43/100, Batch 240/274, Loss: 2.2937
Epoch 43/100, Batch 250/274, Loss: 2.2915
Epoch 43/100, Batch 260/274, Loss: 2.3030
Epoch 43/100, Batch 270/274, Loss: 2.2816
New best model with validation loss: 2.2097, perplexity: 9.11
Epoch 43/100, Loss: 2.2654, Perplexity: 9.63, Val Loss: 2.2097, Val Perplexity: 9.11, Time: 368.44s
Epoch 44/100, Batch 10/274, Loss: 2.3107
Epoch 44/100, Batch 20/274, Loss: 2.3049
Epoch 44/100, Batch 30/274, Loss: 2.2426
Epoch 44/100, Batch 40/274, Loss: 2.2462
Epoch 44/100, Batch 50/274, Loss: 2.2318
Epoch 44/100, Batch 60/274, Loss: 2.2927
Epoch 44/100, Batch 70/274, Loss: 2.2404
Epoch 44/100, Batch 80/274, Loss: 2.2305
Epoch 44/100, Batch 90/274, Loss: 2.1876
Epoch 44/100, Batch 100/274, Loss: 2.2411
Epoch 44/100, Batch 110/274, Loss: 2.3029
Epoch 44/100, Batch 120/274, Loss: 2.2443
Epoch 44/100, Batch 130/274, Loss: 2.2170
Epoch 44/100, Batch 140/274, Loss: 2.2329
Epoch 44/100, Batch 150/274, Loss: 2.2039
Epoch 44/100, Batch 160/274, Loss: 2.2270
Epoch 44/100, Batch 170/274, Loss: 2.2705
Epoch 44/100, Batch 180/274, Loss: 2.2845
Epoch 44/100, Batch 190/274, Loss: 2.3059
Epoch 44/100, Batch 200/274, Loss: 2.2334
Epoch 44/100, Batch 210/274, Loss: 2.3011
Epoch 44/100, Batch 220/274, Loss: 2.2318
Epoch 44/100, Batch 230/274, Loss: 2.2909
Epoch 44/100, Batch 240/274, Loss: 2.2928
Epoch 44/100, Batch 250/274, Loss: 2.2829
Epoch 44/100, Batch 260/274, Loss: 2.3019
Epoch 44/100, Batch 270/274, Loss: 2.2633
New best model with validation loss: 2.2051, perplexity: 9.07
Epoch 44/100, Loss: 2.2616, Perplexity: 9.60, Val Loss: 2.2051, Val Perplexity: 9.07, Time: 367.77s
Epoch 45/100, Batch 10/274, Loss: 2.3489
Epoch 45/100, Batch 20/274, Loss: 2.2671
Epoch 45/100, Batch 30/274, Loss: 2.3174
Epoch 45/100, Batch 40/274, Loss: 2.2379
Epoch 45/100, Batch 50/274, Loss: 2.2324
Epoch 45/100, Batch 60/274, Loss: 2.2833
Epoch 45/100, Batch 70/274, Loss: 2.2394
Epoch 45/100, Batch 80/274, Loss: 2.2151
Epoch 45/100, Batch 90/274, Loss: 2.1905
Epoch 45/100, Batch 100/274, Loss: 2.2089
Epoch 45/100, Batch 110/274, Loss: 2.2438
Epoch 45/100, Batch 120/274, Loss: 2.2762
Epoch 45/100, Batch 130/274, Loss: 2.2116
Epoch 45/100, Batch 140/274, Loss: 2.2349
Epoch 45/100, Batch 150/274, Loss: 2.2158
Epoch 45/100, Batch 160/274, Loss: 2.2218
Epoch 45/100, Batch 170/274, Loss: 2.2642
Epoch 45/100, Batch 180/274, Loss: 2.2749
Epoch 45/100, Batch 190/274, Loss: 2.2969
Epoch 45/100, Batch 200/274, Loss: 2.2273
Epoch 45/100, Batch 210/274, Loss: 2.2589
Epoch 45/100, Batch 220/274, Loss: 2.2224
Epoch 45/100, Batch 230/274, Loss: 2.2760
Epoch 45/100, Batch 240/274, Loss: 2.2830
Epoch 45/100, Batch 250/274, Loss: 2.2755
Epoch 45/100, Batch 260/274, Loss: 2.2944
Epoch 45/100, Batch 270/274, Loss: 2.2648
New best model with validation loss: 2.2038, perplexity: 9.06
Epoch 45/100, Loss: 2.2543, Perplexity: 9.53, Val Loss: 2.2038, Val Perplexity: 9.06, Time: 368.75s
Epoch 46/100, Batch 10/274, Loss: 2.3045
Epoch 46/100, Batch 20/274, Loss: 2.2574
Epoch 46/100, Batch 30/274, Loss: 2.2370
Epoch 46/100, Batch 40/274, Loss: 2.2358
Epoch 46/100, Batch 50/274, Loss: 2.2147
Epoch 46/100, Batch 60/274, Loss: 2.2769
Epoch 46/100, Batch 70/274, Loss: 2.2339
Epoch 46/100, Batch 80/274, Loss: 2.2111
Epoch 46/100, Batch 90/274, Loss: 2.2457
Epoch 46/100, Batch 100/274, Loss: 2.2083
Epoch 46/100, Batch 110/274, Loss: 2.2368
Epoch 46/100, Batch 120/274, Loss: 2.2356
Epoch 46/100, Batch 130/274, Loss: 2.2109
Epoch 46/100, Batch 140/274, Loss: 2.2269
Epoch 46/100, Batch 150/274, Loss: 2.2411
Epoch 46/100, Batch 160/274, Loss: 2.2242
Epoch 46/100, Batch 170/274, Loss: 2.2724
Epoch 46/100, Batch 180/274, Loss: 2.2793
Epoch 46/100, Batch 190/274, Loss: 2.2772
Epoch 46/100, Batch 200/274, Loss: 2.2243
Epoch 46/100, Batch 210/274, Loss: 2.2592
Epoch 46/100, Batch 220/274, Loss: 2.2148
Epoch 46/100, Batch 230/274, Loss: 2.2776
Epoch 46/100, Batch 240/274, Loss: 2.2849
Epoch 46/100, Batch 250/274, Loss: 2.2688
Epoch 46/100, Batch 260/274, Loss: 2.2841
Epoch 46/100, Batch 270/274, Loss: 2.3144
New best model with validation loss: 2.1954, perplexity: 8.98
Epoch 46/100, Loss: 2.2481, Perplexity: 9.47, Val Loss: 2.1954, Val Perplexity: 8.98, Time: 368.55s
Epoch 47/100, Batch 10/274, Loss: 2.2901
Epoch 47/100, Batch 20/274, Loss: 2.2945
Epoch 47/100, Batch 30/274, Loss: 2.2402
Epoch 47/100, Batch 40/274, Loss: 2.2750
Epoch 47/100, Batch 50/274, Loss: 2.2147
Epoch 47/100, Batch 60/274, Loss: 2.2741
Epoch 47/100, Batch 70/274, Loss: 2.2354
Epoch 47/100, Batch 80/274, Loss: 2.2107
Epoch 47/100, Batch 90/274, Loss: 2.1847
Epoch 47/100, Batch 100/274, Loss: 2.1850
Epoch 47/100, Batch 110/274, Loss: 2.2409
Epoch 47/100, Batch 120/274, Loss: 2.2490
Epoch 47/100, Batch 130/274, Loss: 2.2022
Epoch 47/100, Batch 140/274, Loss: 2.2140
Epoch 47/100, Batch 150/274, Loss: 2.1975
Epoch 47/100, Batch 160/274, Loss: 2.2443
Epoch 47/100, Batch 170/274, Loss: 2.2605
Epoch 47/100, Batch 180/274, Loss: 2.2668
Epoch 47/100, Batch 190/274, Loss: 2.2769
Epoch 47/100, Batch 200/274, Loss: 2.2201
Epoch 47/100, Batch 210/274, Loss: 2.2562
Epoch 47/100, Batch 220/274, Loss: 2.2082
Epoch 47/100, Batch 230/274, Loss: 2.2677
Epoch 47/100, Batch 240/274, Loss: 2.2806
Epoch 47/100, Batch 250/274, Loss: 2.2804
Epoch 47/100, Batch 260/274, Loss: 2.3476
Epoch 47/100, Batch 270/274, Loss: 2.2592
New best model with validation loss: 2.1885, perplexity: 8.92
Epoch 47/100, Loss: 2.2456, Perplexity: 9.45, Val Loss: 2.1885, Val Perplexity: 8.92, Time: 367.46s
Epoch 48/100, Batch 10/274, Loss: 2.2870
Epoch 48/100, Batch 20/274, Loss: 2.2659
Epoch 48/100, Batch 30/274, Loss: 2.2249
Epoch 48/100, Batch 40/274, Loss: 2.2245
Epoch 48/100, Batch 50/274, Loss: 2.2143
Epoch 48/100, Batch 60/274, Loss: 2.2692
Epoch 48/100, Batch 70/274, Loss: 2.2250
Epoch 48/100, Batch 80/274, Loss: 2.2096
Epoch 48/100, Batch 90/274, Loss: 2.1757
Epoch 48/100, Batch 100/274, Loss: 2.1935
Epoch 48/100, Batch 110/274, Loss: 2.2302
Epoch 48/100, Batch 120/274, Loss: 2.2302
Epoch 48/100, Batch 130/274, Loss: 2.1987
Epoch 48/100, Batch 140/274, Loss: 2.2163
Epoch 48/100, Batch 150/274, Loss: 2.1936
Epoch 48/100, Batch 160/274, Loss: 2.2019
Epoch 48/100, Batch 170/274, Loss: 2.2430
Epoch 48/100, Batch 180/274, Loss: 2.2619
Epoch 48/100, Batch 190/274, Loss: 2.3159
Epoch 48/100, Batch 200/274, Loss: 2.2162
Epoch 48/100, Batch 210/274, Loss: 2.2497
Epoch 48/100, Batch 220/274, Loss: 2.2146
Epoch 48/100, Batch 230/274, Loss: 2.2654
Epoch 48/100, Batch 240/274, Loss: 2.2697
Epoch 48/100, Batch 250/274, Loss: 2.2651
Epoch 48/100, Batch 260/274, Loss: 2.2908
Epoch 48/100, Batch 270/274, Loss: 2.2996
New best model with validation loss: 2.1862, perplexity: 8.90
Epoch 48/100, Loss: 2.2400, Perplexity: 9.39, Val Loss: 2.1862, Val Perplexity: 8.90, Time: 367.39s
Epoch 49/100, Batch 10/274, Loss: 2.2776
Epoch 49/100, Batch 20/274, Loss: 2.2475
Epoch 49/100, Batch 30/274, Loss: 2.2192
Epoch 49/100, Batch 40/274, Loss: 2.2162
Epoch 49/100, Batch 50/274, Loss: 2.2134
Epoch 49/100, Batch 60/274, Loss: 2.2649
Epoch 49/100, Batch 70/274, Loss: 2.2201
Epoch 49/100, Batch 80/274, Loss: 2.1956
Epoch 49/100, Batch 90/274, Loss: 2.1658
Epoch 49/100, Batch 100/274, Loss: 2.2008
Epoch 49/100, Batch 110/274, Loss: 2.2256
Epoch 49/100, Batch 120/274, Loss: 2.2989
Epoch 49/100, Batch 130/274, Loss: 2.1978
Epoch 49/100, Batch 140/274, Loss: 2.2193
Epoch 49/100, Batch 150/274, Loss: 2.1798
Epoch 49/100, Batch 160/274, Loss: 2.2072
Epoch 49/100, Batch 170/274, Loss: 2.2397
Epoch 49/100, Batch 180/274, Loss: 2.2622
Epoch 49/100, Batch 190/274, Loss: 2.2613
Epoch 49/100, Batch 200/274, Loss: 2.2035
Epoch 49/100, Batch 210/274, Loss: 2.2451
Epoch 49/100, Batch 220/274, Loss: 2.1923
Epoch 49/100, Batch 230/274, Loss: 2.2595
Epoch 49/100, Batch 240/274, Loss: 2.2960
Epoch 49/100, Batch 250/274, Loss: 2.2912
Epoch 49/100, Batch 260/274, Loss: 2.2706
Epoch 49/100, Batch 270/274, Loss: 2.2883
Epoch 49/100, Loss: 2.2343, Perplexity: 9.34, Val Loss: 2.1877, Val Perplexity: 8.91, Time: 369.29s
Epoch 50/100, Batch 10/274, Loss: 2.2999
Epoch 50/100, Batch 20/274, Loss: 2.2405
Epoch 50/100, Batch 30/274, Loss: 2.2119
Epoch 50/100, Batch 40/274, Loss: 2.2124
Epoch 50/100, Batch 50/274, Loss: 2.1959
Epoch 50/100, Batch 60/274, Loss: 2.2609
Epoch 50/100, Batch 70/274, Loss: 2.2160
Epoch 50/100, Batch 80/274, Loss: 2.2004
Epoch 50/100, Batch 90/274, Loss: 2.1612
Epoch 50/100, Batch 100/274, Loss: 2.1808
Epoch 50/100, Batch 110/274, Loss: 2.2302
Epoch 50/100, Batch 120/274, Loss: 2.2883
Epoch 50/100, Batch 130/274, Loss: 2.1874
Epoch 50/100, Batch 140/274, Loss: 2.2042
Epoch 50/100, Batch 150/274, Loss: 2.1867
Epoch 50/100, Batch 160/274, Loss: 2.1915
Epoch 50/100, Batch 170/274, Loss: 2.2388
Epoch 50/100, Batch 180/274, Loss: 2.2701
Epoch 50/100, Batch 190/274, Loss: 2.2609
Epoch 50/100, Batch 200/274, Loss: 2.1961
Epoch 50/100, Batch 210/274, Loss: 2.2444
Epoch 50/100, Batch 220/274, Loss: 2.1913
Epoch 50/100, Batch 230/274, Loss: 2.2686
Epoch 50/100, Batch 240/274, Loss: 2.2645
Epoch 50/100, Batch 250/274, Loss: 2.2651
Epoch 50/100, Batch 260/274, Loss: 2.2667
Epoch 50/100, Batch 270/274, Loss: 2.2448
New best model with validation loss: 2.1833, perplexity: 8.88
Epoch 50/100, Loss: 2.2277, Perplexity: 9.28, Val Loss: 2.1833, Val Perplexity: 8.88, Time: 370.16s
Epoch 51/100, Batch 10/274, Loss: 2.2843
Epoch 51/100, Batch 20/274, Loss: 2.2344
Epoch 51/100, Batch 30/274, Loss: 2.2035
Epoch 51/100, Batch 40/274, Loss: 2.2096
Epoch 51/100, Batch 50/274, Loss: 2.1970
Epoch 51/100, Batch 60/274, Loss: 2.2539
Epoch 51/100, Batch 70/274, Loss: 2.2174
Epoch 51/100, Batch 80/274, Loss: 2.1936
Epoch 51/100, Batch 90/274, Loss: 2.1774
Epoch 51/100, Batch 100/274, Loss: 2.1790
Epoch 51/100, Batch 110/274, Loss: 2.2408
Epoch 51/100, Batch 120/274, Loss: 2.2147
Epoch 51/100, Batch 130/274, Loss: 2.1768
Epoch 51/100, Batch 140/274, Loss: 2.2005
Epoch 51/100, Batch 150/274, Loss: 2.1832
Epoch 51/100, Batch 160/274, Loss: 2.2687
Epoch 51/100, Batch 170/274, Loss: 2.2322
Epoch 51/100, Batch 180/274, Loss: 2.2488
Epoch 51/100, Batch 190/274, Loss: 2.2540
Epoch 51/100, Batch 200/274, Loss: 2.1983
Epoch 51/100, Batch 210/274, Loss: 2.2410
Epoch 51/100, Batch 220/274, Loss: 2.1911
Epoch 51/100, Batch 230/274, Loss: 2.2501
Epoch 51/100, Batch 240/274, Loss: 2.2597
Epoch 51/100, Batch 250/274, Loss: 2.2542
Epoch 51/100, Batch 260/274, Loss: 2.2929
Epoch 51/100, Batch 270/274, Loss: 2.2628
New best model with validation loss: 2.1777, perplexity: 8.83
Epoch 51/100, Loss: 2.2245, Perplexity: 9.25, Val Loss: 2.1777, Val Perplexity: 8.83, Time: 369.34s
Epoch 52/100, Batch 10/274, Loss: 2.2601
Epoch 52/100, Batch 20/274, Loss: 2.2374
Epoch 52/100, Batch 30/274, Loss: 2.2035
Epoch 52/100, Batch 40/274, Loss: 2.2122
Epoch 52/100, Batch 50/274, Loss: 2.1887
Epoch 52/100, Batch 60/274, Loss: 2.2569
Epoch 52/100, Batch 70/274, Loss: 2.2140
Epoch 52/100, Batch 80/274, Loss: 2.1875
Epoch 52/100, Batch 90/274, Loss: 2.1486
Epoch 52/100, Batch 100/274, Loss: 2.1735
Epoch 52/100, Batch 110/274, Loss: 2.2223
Epoch 52/100, Batch 120/274, Loss: 2.2121
Epoch 52/100, Batch 130/274, Loss: 2.1735
Epoch 52/100, Batch 140/274, Loss: 2.2090
Epoch 52/100, Batch 150/274, Loss: 2.1732
Epoch 52/100, Batch 160/274, Loss: 2.1829
Epoch 52/100, Batch 170/274, Loss: 2.2354
Epoch 52/100, Batch 180/274, Loss: 2.2459
Epoch 52/100, Batch 190/274, Loss: 2.2467
Epoch 52/100, Batch 200/274, Loss: 2.1949
Epoch 52/100, Batch 210/274, Loss: 2.2344
Epoch 52/100, Batch 220/274, Loss: 2.1824
Epoch 52/100, Batch 230/274, Loss: 2.2479
Epoch 52/100, Batch 240/274, Loss: 2.2551
Epoch 52/100, Batch 250/274, Loss: 2.2583
Epoch 52/100, Batch 260/274, Loss: 2.2935
Epoch 52/100, Batch 270/274, Loss: 2.2310
New best model with validation loss: 2.1745, perplexity: 8.80
Epoch 52/100, Loss: 2.2203, Perplexity: 9.21, Val Loss: 2.1745, Val Perplexity: 8.80, Time: 369.03s
Epoch 53/100, Batch 10/274, Loss: 2.3143
Epoch 53/100, Batch 20/274, Loss: 2.2526
Epoch 53/100, Batch 30/274, Loss: 2.2065
Epoch 53/100, Batch 40/274, Loss: 2.2112
Epoch 53/100, Batch 50/274, Loss: 2.2010
Epoch 53/100, Batch 60/274, Loss: 2.2349
Epoch 53/100, Batch 70/274, Loss: 2.1979
Epoch 53/100, Batch 80/274, Loss: 2.1837
Epoch 53/100, Batch 90/274, Loss: 2.1497
Epoch 53/100, Batch 100/274, Loss: 2.2503
Epoch 53/100, Batch 110/274, Loss: 2.2160
Epoch 53/100, Batch 120/274, Loss: 2.2055
Epoch 53/100, Batch 130/274, Loss: 2.1776
Epoch 53/100, Batch 140/274, Loss: 2.1859
Epoch 53/100, Batch 150/274, Loss: 2.1882
Epoch 53/100, Batch 160/274, Loss: 2.1957
Epoch 53/100, Batch 170/274, Loss: 2.2240
Epoch 53/100, Batch 180/274, Loss: 2.2542
Epoch 53/100, Batch 190/274, Loss: 2.2748
Epoch 53/100, Batch 200/274, Loss: 2.1867
Epoch 53/100, Batch 210/274, Loss: 2.2468
Epoch 53/100, Batch 220/274, Loss: 2.1700
Epoch 53/100, Batch 230/274, Loss: 2.2420
Epoch 53/100, Batch 240/274, Loss: 2.2521
Epoch 53/100, Batch 250/274, Loss: 2.2419
Epoch 53/100, Batch 260/274, Loss: 2.2648
Epoch 53/100, Batch 270/274, Loss: 2.2248
Epoch 53/100, Loss: 2.2182, Perplexity: 9.19, Val Loss: 2.1748, Val Perplexity: 8.80, Time: 367.54s
Epoch 54/100, Batch 10/274, Loss: 2.2619
Epoch 54/100, Batch 20/274, Loss: 2.2238
Epoch 54/100, Batch 30/274, Loss: 2.1981
Epoch 54/100, Batch 40/274, Loss: 2.1999
Epoch 54/100, Batch 50/274, Loss: 2.1851
Epoch 54/100, Batch 60/274, Loss: 2.2384
Epoch 54/100, Batch 70/274, Loss: 2.1960
Epoch 54/100, Batch 80/274, Loss: 2.2065
Epoch 54/100, Batch 90/274, Loss: 2.1523
Epoch 54/100, Batch 100/274, Loss: 2.1624
Epoch 54/100, Batch 110/274, Loss: 2.2150
Epoch 54/100, Batch 120/274, Loss: 2.2050
Epoch 54/100, Batch 130/274, Loss: 2.1660
Epoch 54/100, Batch 140/274, Loss: 2.1797
Epoch 54/100, Batch 150/274, Loss: 2.1880
Epoch 54/100, Batch 160/274, Loss: 2.1917
Epoch 54/100, Batch 170/274, Loss: 2.2241
Epoch 54/100, Batch 180/274, Loss: 2.2642
Epoch 54/100, Batch 190/274, Loss: 2.2427
Epoch 54/100, Batch 200/274, Loss: 2.1899
Epoch 54/100, Batch 210/274, Loss: 2.2217
Epoch 54/100, Batch 220/274, Loss: 2.1819
Epoch 54/100, Batch 230/274, Loss: 2.2480
Epoch 54/100, Batch 240/274, Loss: 2.3177
Epoch 54/100, Batch 250/274, Loss: 2.2348
Epoch 54/100, Batch 260/274, Loss: 2.3348
Epoch 54/100, Batch 270/274, Loss: 2.2195
New best model with validation loss: 2.1730, perplexity: 8.78
Epoch 54/100, Loss: 2.2154, Perplexity: 9.16, Val Loss: 2.1730, Val Perplexity: 8.78, Time: 366.68s
Epoch 55/100, Batch 10/274, Loss: 2.2560
Epoch 55/100, Batch 20/274, Loss: 2.2225
Epoch 55/100, Batch 30/274, Loss: 2.1937
Epoch 55/100, Batch 40/274, Loss: 2.2017
Epoch 55/100, Batch 50/274, Loss: 2.2055
Epoch 55/100, Batch 60/274, Loss: 2.2322
Epoch 55/100, Batch 70/274, Loss: 2.1896
Epoch 55/100, Batch 80/274, Loss: 2.1775
Epoch 55/100, Batch 90/274, Loss: 2.1409
Epoch 55/100, Batch 100/274, Loss: 2.1589
Epoch 55/100, Batch 110/274, Loss: 2.2377
Epoch 55/100, Batch 120/274, Loss: 2.2074
Epoch 55/100, Batch 130/274, Loss: 2.1624
Epoch 55/100, Batch 140/274, Loss: 2.1837
Epoch 55/100, Batch 150/274, Loss: 2.1663
Epoch 55/100, Batch 160/274, Loss: 2.1755
Epoch 55/100, Batch 170/274, Loss: 2.2233
Epoch 55/100, Batch 180/274, Loss: 2.2620
Epoch 55/100, Batch 190/274, Loss: 2.2394
Epoch 55/100, Batch 200/274, Loss: 2.1860
Epoch 55/100, Batch 210/274, Loss: 2.2243
Epoch 55/100, Batch 220/274, Loss: 2.1758
Epoch 55/100, Batch 230/274, Loss: 2.2827
Epoch 55/100, Batch 240/274, Loss: 2.2379
Epoch 55/100, Batch 250/274, Loss: 2.2355
Epoch 55/100, Batch 260/274, Loss: 2.2565
Epoch 55/100, Batch 270/274, Loss: 2.2196
New best model with validation loss: 2.1690, perplexity: 8.75
Epoch 55/100, Loss: 2.2105, Perplexity: 9.12, Val Loss: 2.1690, Val Perplexity: 8.75, Time: 367.97s
Epoch 56/100, Batch 10/274, Loss: 2.2632
Epoch 56/100, Batch 20/274, Loss: 2.2171
Epoch 56/100, Batch 30/274, Loss: 2.1935
Epoch 56/100, Batch 40/274, Loss: 2.1952
Epoch 56/100, Batch 50/274, Loss: 2.1769
Epoch 56/100, Batch 60/274, Loss: 2.2424
Epoch 56/100, Batch 70/274, Loss: 2.1870
Epoch 56/100, Batch 80/274, Loss: 2.2732
Epoch 56/100, Batch 90/274, Loss: 2.1357
Epoch 56/100, Batch 100/274, Loss: 2.1583
Epoch 56/100, Batch 110/274, Loss: 2.2105
Epoch 56/100, Batch 120/274, Loss: 2.1898
Epoch 56/100, Batch 130/274, Loss: 2.1762
Epoch 56/100, Batch 140/274, Loss: 2.2968
Epoch 56/100, Batch 150/274, Loss: 2.1575
Epoch 56/100, Batch 160/274, Loss: 2.1839
Epoch 56/100, Batch 170/274, Loss: 2.2203
Epoch 56/100, Batch 180/274, Loss: 2.2329
Epoch 56/100, Batch 190/274, Loss: 2.2657
Epoch 56/100, Batch 200/274, Loss: 2.2288
Epoch 56/100, Batch 210/274, Loss: 2.2219
Epoch 56/100, Batch 220/274, Loss: 2.1699
Epoch 56/100, Batch 230/274, Loss: 2.3063
Epoch 56/100, Batch 240/274, Loss: 2.2443
Epoch 56/100, Batch 250/274, Loss: 2.2309
Epoch 56/100, Batch 260/274, Loss: 2.2734
Epoch 56/100, Batch 270/274, Loss: 2.2116
New best model with validation loss: 2.1662, perplexity: 8.73
Epoch 56/100, Loss: 2.2077, Perplexity: 9.09, Val Loss: 2.1662, Val Perplexity: 8.73, Time: 367.86s
Epoch 57/100, Batch 10/274, Loss: 2.2490
Epoch 57/100, Batch 20/274, Loss: 2.2147
Epoch 57/100, Batch 30/274, Loss: 2.1899
Epoch 57/100, Batch 40/274, Loss: 2.1989
Epoch 57/100, Batch 50/274, Loss: 2.2022
Epoch 57/100, Batch 60/274, Loss: 2.2328
Epoch 57/100, Batch 70/274, Loss: 2.1858
Epoch 57/100, Batch 80/274, Loss: 2.1707
Epoch 57/100, Batch 90/274, Loss: 2.1532
Epoch 57/100, Batch 100/274, Loss: 2.1586
Epoch 57/100, Batch 110/274, Loss: 2.2553
Epoch 57/100, Batch 120/274, Loss: 2.1924
Epoch 57/100, Batch 130/274, Loss: 2.1601
Epoch 57/100, Batch 140/274, Loss: 2.2369
Epoch 57/100, Batch 150/274, Loss: 2.1588
Epoch 57/100, Batch 160/274, Loss: 2.1778
Epoch 57/100, Batch 170/274, Loss: 2.2097
Epoch 57/100, Batch 180/274, Loss: 2.2382
Epoch 57/100, Batch 190/274, Loss: 2.2292
Epoch 57/100, Batch 200/274, Loss: 2.1727
Epoch 57/100, Batch 210/274, Loss: 2.2134
Epoch 57/100, Batch 220/274, Loss: 2.1691
Epoch 57/100, Batch 230/274, Loss: 2.2316
Epoch 57/100, Batch 240/274, Loss: 2.2348
Epoch 57/100, Batch 250/274, Loss: 2.2397
Epoch 57/100, Batch 260/274, Loss: 2.2412
Epoch 57/100, Batch 270/274, Loss: 2.2216
Epoch 57/100, Loss: 2.2033, Perplexity: 9.06, Val Loss: 2.1682, Val Perplexity: 8.74, Time: 368.75s
Epoch 58/100, Batch 10/274, Loss: 2.2414
Epoch 58/100, Batch 20/274, Loss: 2.2140
Epoch 58/100, Batch 30/274, Loss: 2.1814
Epoch 58/100, Batch 40/274, Loss: 2.1852
Epoch 58/100, Batch 50/274, Loss: 2.1669
Epoch 58/100, Batch 60/274, Loss: 2.2221
Epoch 58/100, Batch 70/274, Loss: 2.1843
Epoch 58/100, Batch 80/274, Loss: 2.1584
Epoch 58/100, Batch 90/274, Loss: 2.1362
Epoch 58/100, Batch 100/274, Loss: 2.1515
Epoch 58/100, Batch 110/274, Loss: 2.2262
Epoch 58/100, Batch 120/274, Loss: 2.2044
Epoch 58/100, Batch 130/274, Loss: 2.1562
Epoch 58/100, Batch 140/274, Loss: 2.1756
Epoch 58/100, Batch 150/274, Loss: 2.1495
Epoch 58/100, Batch 160/274, Loss: 2.1649
Epoch 58/100, Batch 170/274, Loss: 2.2799
Epoch 58/100, Batch 180/274, Loss: 2.2495
Epoch 58/100, Batch 190/274, Loss: 2.2321
Epoch 58/100, Batch 200/274, Loss: 2.1759
Epoch 58/100, Batch 210/274, Loss: 2.2141
Epoch 58/100, Batch 220/274, Loss: 2.2015
Epoch 58/100, Batch 230/274, Loss: 2.2252
Epoch 58/100, Batch 240/274, Loss: 2.2278
Epoch 58/100, Batch 250/274, Loss: 2.2185
Epoch 58/100, Batch 260/274, Loss: 2.2607
Epoch 58/100, Batch 270/274, Loss: 2.1978
New best model with validation loss: 2.1618, perplexity: 8.69
Epoch 58/100, Loss: 2.2001, Perplexity: 9.03, Val Loss: 2.1618, Val Perplexity: 8.69, Time: 367.95s
Epoch 59/100, Batch 10/274, Loss: 2.2930
Epoch 59/100, Batch 20/274, Loss: 2.2065
Epoch 59/100, Batch 30/274, Loss: 2.1895
Epoch 59/100, Batch 40/274, Loss: 2.1853
Epoch 59/100, Batch 50/274, Loss: 2.2396
Epoch 59/100, Batch 60/274, Loss: 2.2322
Epoch 59/100, Batch 70/274, Loss: 2.1938
Epoch 59/100, Batch 80/274, Loss: 2.1668
Epoch 59/100, Batch 90/274, Loss: 2.1291
Epoch 59/100, Batch 100/274, Loss: 2.1383
Epoch 59/100, Batch 110/274, Loss: 2.2179
Epoch 59/100, Batch 120/274, Loss: 2.1786
Epoch 59/100, Batch 130/274, Loss: 2.2326
Epoch 59/100, Batch 140/274, Loss: 2.1659
Epoch 59/100, Batch 150/274, Loss: 2.1443
Epoch 59/100, Batch 160/274, Loss: 2.1587
Epoch 59/100, Batch 170/274, Loss: 2.2074
Epoch 59/100, Batch 180/274, Loss: 2.2200
Epoch 59/100, Batch 190/274, Loss: 2.2272
Epoch 59/100, Batch 200/274, Loss: 2.1818
Epoch 59/100, Batch 210/274, Loss: 2.2103
Epoch 59/100, Batch 220/274, Loss: 2.1655
Epoch 59/100, Batch 230/274, Loss: 2.2256
Epoch 59/100, Batch 240/274, Loss: 2.2257
Epoch 59/100, Batch 250/274, Loss: 2.2265
Epoch 59/100, Batch 260/274, Loss: 2.2295
Epoch 59/100, Batch 270/274, Loss: 2.2048
New best model with validation loss: 2.1600, perplexity: 8.67
Epoch 59/100, Loss: 2.1965, Perplexity: 8.99, Val Loss: 2.1600, Val Perplexity: 8.67, Time: 367.46s
Epoch 60/100, Batch 10/274, Loss: 2.2459
Epoch 60/100, Batch 20/274, Loss: 2.1993
Epoch 60/100, Batch 30/274, Loss: 2.1729
Epoch 60/100, Batch 40/274, Loss: 2.1836
Epoch 60/100, Batch 50/274, Loss: 2.1634
Epoch 60/100, Batch 60/274, Loss: 2.2183
Epoch 60/100, Batch 70/274, Loss: 2.2154
Epoch 60/100, Batch 80/274, Loss: 2.1550
Epoch 60/100, Batch 90/274, Loss: 2.1293
Epoch 60/100, Batch 100/274, Loss: 2.1385
Epoch 60/100, Batch 110/274, Loss: 2.1897
Epoch 60/100, Batch 120/274, Loss: 2.2256
Epoch 60/100, Batch 130/274, Loss: 2.1495
Epoch 60/100, Batch 140/274, Loss: 2.1927
Epoch 60/100, Batch 150/274, Loss: 2.1475
Epoch 60/100, Batch 160/274, Loss: 2.1619
Epoch 60/100, Batch 170/274, Loss: 2.1982
Epoch 60/100, Batch 180/274, Loss: 2.2560
Epoch 60/100, Batch 190/274, Loss: 2.2950
Epoch 60/100, Batch 200/274, Loss: 2.1664
Epoch 60/100, Batch 210/274, Loss: 2.2017
Epoch 60/100, Batch 220/274, Loss: 2.1581
Epoch 60/100, Batch 230/274, Loss: 2.2135
Epoch 60/100, Batch 240/274, Loss: 2.2236
Epoch 60/100, Batch 250/274, Loss: 2.2179
Epoch 60/100, Batch 260/274, Loss: 2.2438
Epoch 60/100, Batch 270/274, Loss: 2.2058
Epoch 60/100, Loss: 2.1925, Perplexity: 8.96, Val Loss: 2.1624, Val Perplexity: 8.69, Time: 368.68s
Epoch 61/100, Batch 10/274, Loss: 2.2379
Epoch 61/100, Batch 20/274, Loss: 2.1963
Epoch 61/100, Batch 30/274, Loss: 2.1738
Epoch 61/100, Batch 40/274, Loss: 2.1706
Epoch 61/100, Batch 50/274, Loss: 2.1611
Epoch 61/100, Batch 60/274, Loss: 2.2133
Epoch 61/100, Batch 70/274, Loss: 2.1756
Epoch 61/100, Batch 80/274, Loss: 2.1569
Epoch 61/100, Batch 90/274, Loss: 2.1384
Epoch 61/100, Batch 100/274, Loss: 2.1415
Epoch 61/100, Batch 110/274, Loss: 2.1843
Epoch 61/100, Batch 120/274, Loss: 2.1780
Epoch 61/100, Batch 130/274, Loss: 2.1489
Epoch 61/100, Batch 140/274, Loss: 2.1614
Epoch 61/100, Batch 150/274, Loss: 2.1917
Epoch 61/100, Batch 160/274, Loss: 2.1554
Epoch 61/100, Batch 170/274, Loss: 2.1980
Epoch 61/100, Batch 180/274, Loss: 2.2166
Epoch 61/100, Batch 190/274, Loss: 2.2139
Epoch 61/100, Batch 200/274, Loss: 2.2164
Epoch 61/100, Batch 210/274, Loss: 2.2065
Epoch 61/100, Batch 220/274, Loss: 2.1499
Epoch 61/100, Batch 230/274, Loss: 2.2118
Epoch 61/100, Batch 240/274, Loss: 2.2900
Epoch 61/100, Batch 250/274, Loss: 2.2162
Epoch 61/100, Batch 260/274, Loss: 2.2292
Epoch 61/100, Batch 270/274, Loss: 2.2448
New best model with validation loss: 2.1590, perplexity: 8.66
Epoch 61/100, Loss: 2.1893, Perplexity: 8.93, Val Loss: 2.1590, Val Perplexity: 8.66, Time: 369.59s
Epoch 62/100, Batch 10/274, Loss: 2.2308
Epoch 62/100, Batch 20/274, Loss: 2.1991
Epoch 62/100, Batch 30/274, Loss: 2.1638
Epoch 62/100, Batch 40/274, Loss: 2.1721
Epoch 62/100, Batch 50/274, Loss: 2.1577
Epoch 62/100, Batch 60/274, Loss: 2.2089
Epoch 62/100, Batch 70/274, Loss: 2.1704
Epoch 62/100, Batch 80/274, Loss: 2.1803
Epoch 62/100, Batch 90/274, Loss: 2.1464
Epoch 62/100, Batch 100/274, Loss: 2.1507
Epoch 62/100, Batch 110/274, Loss: 2.1829
Epoch 62/100, Batch 120/274, Loss: 2.1748
Epoch 62/100, Batch 130/274, Loss: 2.1415
Epoch 62/100, Batch 140/274, Loss: 2.1794
Epoch 62/100, Batch 150/274, Loss: 2.1426
Epoch 62/100, Batch 160/274, Loss: 2.1883
Epoch 62/100, Batch 170/274, Loss: 2.2046
Epoch 62/100, Batch 180/274, Loss: 2.2123
Epoch 62/100, Batch 190/274, Loss: 2.2450
Epoch 62/100, Batch 200/274, Loss: 2.1551
Epoch 62/100, Batch 210/274, Loss: 2.2005
Epoch 62/100, Batch 220/274, Loss: 2.2583
Epoch 62/100, Batch 230/274, Loss: 2.2355
Epoch 62/100, Batch 240/274, Loss: 2.2162
Epoch 62/100, Batch 250/274, Loss: 2.2158
Epoch 62/100, Batch 260/274, Loss: 2.2530
Epoch 62/100, Batch 270/274, Loss: 2.1994
New best model with validation loss: 2.1566, perplexity: 8.64
Epoch 62/100, Loss: 2.1859, Perplexity: 8.90, Val Loss: 2.1566, Val Perplexity: 8.64, Time: 368.16s
Epoch 63/100, Batch 10/274, Loss: 2.2252
Epoch 63/100, Batch 20/274, Loss: 2.2010
Epoch 63/100, Batch 30/274, Loss: 2.1703
Epoch 63/100, Batch 40/274, Loss: 2.1836
Epoch 63/100, Batch 50/274, Loss: 2.1526
Epoch 63/100, Batch 60/274, Loss: 2.2134
Epoch 63/100, Batch 70/274, Loss: 2.1688
Epoch 63/100, Batch 80/274, Loss: 2.1614
Epoch 63/100, Batch 90/274, Loss: 2.1179
Epoch 63/100, Batch 100/274, Loss: 2.1820
Epoch 63/100, Batch 110/274, Loss: 2.1775
Epoch 63/100, Batch 120/274, Loss: 2.1733
Epoch 63/100, Batch 130/274, Loss: 2.1414
Epoch 63/100, Batch 140/274, Loss: 2.1865
Epoch 63/100, Batch 150/274, Loss: 2.1412
Epoch 63/100, Batch 160/274, Loss: 2.1515
Epoch 63/100, Batch 170/274, Loss: 2.2127
Epoch 63/100, Batch 180/274, Loss: 2.2080
Epoch 63/100, Batch 190/274, Loss: 2.2540
Epoch 63/100, Batch 200/274, Loss: 2.1564
Epoch 63/100, Batch 210/274, Loss: 2.1949
Epoch 63/100, Batch 220/274, Loss: 2.1432
Epoch 63/100, Batch 230/274, Loss: 2.2108
Epoch 63/100, Batch 240/274, Loss: 2.2145
Epoch 63/100, Batch 250/274, Loss: 2.2208
Epoch 63/100, Batch 260/274, Loss: 2.2193
Epoch 63/100, Batch 270/274, Loss: 2.1864
New best model with validation loss: 2.1558, perplexity: 8.63
Epoch 63/100, Loss: 2.1845, Perplexity: 8.89, Val Loss: 2.1558, Val Perplexity: 8.63, Time: 367.00s
Epoch 64/100, Batch 10/274, Loss: 2.2330
Epoch 64/100, Batch 20/274, Loss: 2.2016
Epoch 64/100, Batch 30/274, Loss: 2.1580
Epoch 64/100, Batch 40/274, Loss: 2.1742
Epoch 64/100, Batch 50/274, Loss: 2.1530
Epoch 64/100, Batch 60/274, Loss: 2.2106
Epoch 64/100, Batch 70/274, Loss: 2.1712
Epoch 64/100, Batch 80/274, Loss: 2.1403
Epoch 64/100, Batch 90/274, Loss: 2.1157
Epoch 64/100, Batch 100/274, Loss: 2.1341
Epoch 64/100, Batch 110/274, Loss: 2.1738
Epoch 64/100, Batch 120/274, Loss: 2.1735
Epoch 64/100, Batch 130/274, Loss: 2.1374
Epoch 64/100, Batch 140/274, Loss: 2.1539
Epoch 64/100, Batch 150/274, Loss: 2.1325
Epoch 64/100, Batch 160/274, Loss: 2.1464
Epoch 64/100, Batch 170/274, Loss: 2.1863
Epoch 64/100, Batch 180/274, Loss: 2.2008
Epoch 64/100, Batch 190/274, Loss: 2.2071
Epoch 64/100, Batch 200/274, Loss: 2.1580
Epoch 64/100, Batch 210/274, Loss: 2.1913
Epoch 64/100, Batch 220/274, Loss: 2.1440
Epoch 64/100, Batch 230/274, Loss: 2.2103
Epoch 64/100, Batch 240/274, Loss: 2.2089
Epoch 64/100, Batch 250/274, Loss: 2.2125
Epoch 64/100, Batch 260/274, Loss: 2.2172
Epoch 64/100, Batch 270/274, Loss: 2.1981
Epoch 64/100, Loss: 2.1804, Perplexity: 8.85, Val Loss: 2.1601, Val Perplexity: 8.67, Time: 367.76s
Epoch 65/100, Batch 10/274, Loss: 2.2323
Epoch 65/100, Batch 20/274, Loss: 2.1957
Epoch 65/100, Batch 30/274, Loss: 2.1638
Epoch 65/100, Batch 40/274, Loss: 2.1681
Epoch 65/100, Batch 50/274, Loss: 2.1486
Epoch 65/100, Batch 60/274, Loss: 2.2074
Epoch 65/100, Batch 70/274, Loss: 2.1614
Epoch 65/100, Batch 80/274, Loss: 2.1688
Epoch 65/100, Batch 90/274, Loss: 2.1203
Epoch 65/100, Batch 100/274, Loss: 2.1359
Epoch 65/100, Batch 110/274, Loss: 2.1849
Epoch 65/100, Batch 120/274, Loss: 2.1592
Epoch 65/100, Batch 130/274, Loss: 2.1318
Epoch 65/100, Batch 140/274, Loss: 2.1543
Epoch 65/100, Batch 150/274, Loss: 2.1311
Epoch 65/100, Batch 160/274, Loss: 2.1471
Epoch 65/100, Batch 170/274, Loss: 2.2025
Epoch 65/100, Batch 180/274, Loss: 2.2092
Epoch 65/100, Batch 190/274, Loss: 2.2121
Epoch 65/100, Batch 200/274, Loss: 2.1722
Epoch 65/100, Batch 210/274, Loss: 2.1978
Epoch 65/100, Batch 220/274, Loss: 2.1419
Epoch 65/100, Batch 230/274, Loss: 2.2000
Epoch 65/100, Batch 240/274, Loss: 2.2163
Epoch 65/100, Batch 250/274, Loss: 2.2047
Epoch 65/100, Batch 260/274, Loss: 2.2627
Epoch 65/100, Batch 270/274, Loss: 2.1827
New best model with validation loss: 2.1498, perplexity: 8.58
Epoch 65/100, Loss: 2.1775, Perplexity: 8.82, Val Loss: 2.1498, Val Perplexity: 8.58, Time: 368.53s
Epoch 66/100, Batch 10/274, Loss: 2.2212
Epoch 66/100, Batch 20/274, Loss: 2.1860
Epoch 66/100, Batch 30/274, Loss: 2.1564
Epoch 66/100, Batch 40/274, Loss: 2.1625
Epoch 66/100, Batch 50/274, Loss: 2.1421
Epoch 66/100, Batch 60/274, Loss: 2.1984
Epoch 66/100, Batch 70/274, Loss: 2.1613
Epoch 66/100, Batch 80/274, Loss: 2.1430
Epoch 66/100, Batch 90/274, Loss: 2.1084
Epoch 66/100, Batch 100/274, Loss: 2.1228
Epoch 66/100, Batch 110/274, Loss: 2.2170
Epoch 66/100, Batch 120/274, Loss: 2.1564
Epoch 66/100, Batch 130/274, Loss: 2.1291
Epoch 66/100, Batch 140/274, Loss: 2.1627
Epoch 66/100, Batch 150/274, Loss: 2.1327
Epoch 66/100, Batch 160/274, Loss: 2.1484
Epoch 66/100, Batch 170/274, Loss: 2.1878
Epoch 66/100, Batch 180/274, Loss: 2.1991
Epoch 66/100, Batch 190/274, Loss: 2.2671
Epoch 66/100, Batch 200/274, Loss: 2.1571
Epoch 66/100, Batch 210/274, Loss: 2.1890
Epoch 66/100, Batch 220/274, Loss: 2.1681
Epoch 66/100, Batch 230/274, Loss: 2.2053
Epoch 66/100, Batch 240/274, Loss: 2.2007
Epoch 66/100, Batch 250/274, Loss: 2.2330
Epoch 66/100, Batch 260/274, Loss: 2.2199
Epoch 66/100, Batch 270/274, Loss: 2.1886
Epoch 66/100, Loss: 2.1761, Perplexity: 8.81, Val Loss: 2.1509, Val Perplexity: 8.59, Time: 367.58s
Epoch 67/100, Batch 10/274, Loss: 2.2131
Epoch 67/100, Batch 20/274, Loss: 2.1810
Epoch 67/100, Batch 30/274, Loss: 2.1570
Epoch 67/100, Batch 40/274, Loss: 2.1453
Epoch 67/100, Batch 50/274, Loss: 2.1399
Epoch 67/100, Batch 60/274, Loss: 2.3078
Epoch 67/100, Batch 70/274, Loss: 2.1539
Epoch 67/100, Batch 80/274, Loss: 2.1398
Epoch 67/100, Batch 90/274, Loss: 2.1135
Epoch 67/100, Batch 100/274, Loss: 2.1235
Epoch 67/100, Batch 110/274, Loss: 2.1715
Epoch 67/100, Batch 120/274, Loss: 2.1610
Epoch 67/100, Batch 130/274, Loss: 2.1308
Epoch 67/100, Batch 140/274, Loss: 2.1628
Epoch 67/100, Batch 150/274, Loss: 2.1268
Epoch 67/100, Batch 160/274, Loss: 2.1416
Epoch 67/100, Batch 170/274, Loss: 2.1776
Epoch 67/100, Batch 180/274, Loss: 2.2022
Epoch 67/100, Batch 190/274, Loss: 2.1978
Epoch 67/100, Batch 200/274, Loss: 2.1881
Epoch 67/100, Batch 210/274, Loss: 2.1904
Epoch 67/100, Batch 220/274, Loss: 2.1378
Epoch 67/100, Batch 230/274, Loss: 2.2192
Epoch 67/100, Batch 240/274, Loss: 2.2075
Epoch 67/100, Batch 250/274, Loss: 2.2108
Epoch 67/100, Batch 260/274, Loss: 2.2142
Epoch 67/100, Batch 270/274, Loss: 2.1869
Epoch 67/100, Loss: 2.1727, Perplexity: 8.78, Val Loss: 2.1522, Val Perplexity: 8.60, Time: 367.72s
Epoch 68/100, Batch 10/274, Loss: 2.2204
Epoch 68/100, Batch 20/274, Loss: 2.1863
Epoch 68/100, Batch 30/274, Loss: 2.1528
Epoch 68/100, Batch 40/274, Loss: 2.1542
Epoch 68/100, Batch 50/274, Loss: 2.1482
Epoch 68/100, Batch 60/274, Loss: 2.2520
Epoch 68/100, Batch 70/274, Loss: 2.2276
Epoch 68/100, Batch 80/274, Loss: 2.1529
Epoch 68/100, Batch 90/274, Loss: 2.1087
Epoch 68/100, Batch 100/274, Loss: 2.1241
Epoch 68/100, Batch 110/274, Loss: 2.1675
Epoch 68/100, Batch 120/274, Loss: 2.1514
Epoch 68/100, Batch 130/274, Loss: 2.1581
Epoch 68/100, Batch 140/274, Loss: 2.1442
Epoch 68/100, Batch 150/274, Loss: 2.1323
Epoch 68/100, Batch 160/274, Loss: 2.1394
Epoch 68/100, Batch 170/274, Loss: 2.1827
Epoch 68/100, Batch 180/274, Loss: 2.1973
Epoch 68/100, Batch 190/274, Loss: 2.1993
Epoch 68/100, Batch 200/274, Loss: 2.1965
Epoch 68/100, Batch 210/274, Loss: 2.1993
Epoch 68/100, Batch 220/274, Loss: 2.1791
Epoch 68/100, Batch 230/274, Loss: 2.2168
Epoch 68/100, Batch 240/274, Loss: 2.1970
Epoch 68/100, Batch 250/274, Loss: 2.2004
Epoch 68/100, Batch 260/274, Loss: 2.2268
Epoch 68/100, Batch 270/274, Loss: 2.1744
Epoch 68/100, Loss: 2.1699, Perplexity: 8.76, Val Loss: 2.1505, Val Perplexity: 8.59, Time: 367.74s
Epoch 69/100, Batch 10/274, Loss: 2.2144
Epoch 69/100, Batch 20/274, Loss: 2.1835
Epoch 69/100, Batch 30/274, Loss: 2.1595
Epoch 69/100, Batch 40/274, Loss: 2.2289
Epoch 69/100, Batch 50/274, Loss: 2.1398
Epoch 69/100, Batch 60/274, Loss: 2.1953
Epoch 69/100, Batch 70/274, Loss: 2.1968
Epoch 69/100, Batch 80/274, Loss: 2.1329
Epoch 69/100, Batch 90/274, Loss: 2.1799
Epoch 69/100, Batch 100/274, Loss: 2.1148
Epoch 69/100, Batch 110/274, Loss: 2.1648
Epoch 69/100, Batch 120/274, Loss: 2.1458
Epoch 69/100, Batch 130/274, Loss: 2.1262
Epoch 69/100, Batch 140/274, Loss: 2.1450
Epoch 69/100, Batch 150/274, Loss: 2.1266
Epoch 69/100, Batch 160/274, Loss: 2.1289
Epoch 69/100, Batch 170/274, Loss: 2.1816
Epoch 69/100, Batch 180/274, Loss: 2.1957
Epoch 69/100, Batch 190/274, Loss: 2.1995
Epoch 69/100, Batch 200/274, Loss: 2.1484
Epoch 69/100, Batch 210/274, Loss: 2.1912
Epoch 69/100, Batch 220/274, Loss: 2.1384
Epoch 69/100, Batch 230/274, Loss: 2.2933
Epoch 69/100, Batch 240/274, Loss: 2.2010
Epoch 69/100, Batch 250/274, Loss: 2.2004
Epoch 69/100, Batch 260/274, Loss: 2.2562
Epoch 69/100, Batch 270/274, Loss: 2.1862
Epoch 69/100, Loss: 2.1673, Perplexity: 8.73, Val Loss: 2.1502, Val Perplexity: 8.59, Time: 368.36s
Epoch 70/100, Batch 10/274, Loss: 2.2259
Epoch 70/100, Batch 20/274, Loss: 2.1798
Epoch 70/100, Batch 30/274, Loss: 2.2245
Epoch 70/100, Batch 40/274, Loss: 2.1488
Epoch 70/100, Batch 50/274, Loss: 2.1358
Epoch 70/100, Batch 60/274, Loss: 2.1849
Epoch 70/100, Batch 70/274, Loss: 2.1520
Epoch 70/100, Batch 80/274, Loss: 2.1440
Epoch 70/100, Batch 90/274, Loss: 2.1053
Epoch 70/100, Batch 100/274, Loss: 2.1572
Epoch 70/100, Batch 110/274, Loss: 2.1926
Epoch 70/100, Batch 120/274, Loss: 2.1834
Epoch 70/100, Batch 130/274, Loss: 2.1212
Epoch 70/100, Batch 140/274, Loss: 2.1827
Epoch 70/100, Batch 150/274, Loss: 2.1191
Epoch 70/100, Batch 160/274, Loss: 2.1273
Epoch 70/100, Batch 170/274, Loss: 2.1704
Epoch 70/100, Batch 180/274, Loss: 2.2115
Epoch 70/100, Batch 190/274, Loss: 2.1965
Epoch 70/100, Batch 200/274, Loss: 2.1733
Epoch 70/100, Batch 210/274, Loss: 2.1812
Epoch 70/100, Batch 220/274, Loss: 2.1872
Epoch 70/100, Batch 230/274, Loss: 2.2030
Epoch 70/100, Batch 240/274, Loss: 2.2087
Epoch 70/100, Batch 250/274, Loss: 2.2378
Epoch 70/100, Batch 260/274, Loss: 2.2848
Epoch 70/100, Batch 270/274, Loss: 2.1696
New best model with validation loss: 2.1452, perplexity: 8.54
Epoch 70/100, Loss: 2.1641, Perplexity: 8.71, Val Loss: 2.1452, Val Perplexity: 8.54, Time: 368.54s
Epoch 71/100, Batch 10/274, Loss: 2.2067
Epoch 71/100, Batch 20/274, Loss: 2.1769
Epoch 71/100, Batch 30/274, Loss: 2.1448
Epoch 71/100, Batch 40/274, Loss: 2.1478
Epoch 71/100, Batch 50/274, Loss: 2.1351
Epoch 71/100, Batch 60/274, Loss: 2.1845
Epoch 71/100, Batch 70/274, Loss: 2.1639
Epoch 71/100, Batch 80/274, Loss: 2.1410
Epoch 71/100, Batch 90/274, Loss: 2.0997
Epoch 71/100, Batch 100/274, Loss: 2.1104
Epoch 71/100, Batch 110/274, Loss: 2.2081
Epoch 71/100, Batch 120/274, Loss: 2.1483
Epoch 71/100, Batch 130/274, Loss: 2.1204
Epoch 71/100, Batch 140/274, Loss: 2.1412
Epoch 71/100, Batch 150/274, Loss: 2.1198
Epoch 71/100, Batch 160/274, Loss: 2.1763
Epoch 71/100, Batch 170/274, Loss: 2.1674
Epoch 71/100, Batch 180/274, Loss: 2.1922
Epoch 71/100, Batch 190/274, Loss: 2.1985
Epoch 71/100, Batch 200/274, Loss: 2.1412
Epoch 71/100, Batch 210/274, Loss: 2.2062
Epoch 71/100, Batch 220/274, Loss: 2.1248
Epoch 71/100, Batch 230/274, Loss: 2.1900
Epoch 71/100, Batch 240/274, Loss: 2.1942
Epoch 71/100, Batch 250/274, Loss: 2.1927
Epoch 71/100, Batch 260/274, Loss: 2.2606
Epoch 71/100, Batch 270/274, Loss: 2.1722
New best model with validation loss: 2.1413, perplexity: 8.51
Epoch 71/100, Loss: 2.1630, Perplexity: 8.70, Val Loss: 2.1413, Val Perplexity: 8.51, Time: 367.88s
Epoch 72/100, Batch 10/274, Loss: 2.2113
Epoch 72/100, Batch 20/274, Loss: 2.2982
Epoch 72/100, Batch 30/274, Loss: 2.1512
Epoch 72/100, Batch 40/274, Loss: 2.1475
Epoch 72/100, Batch 50/274, Loss: 2.1304
Epoch 72/100, Batch 60/274, Loss: 2.2651
Epoch 72/100, Batch 70/274, Loss: 2.1617
Epoch 72/100, Batch 80/274, Loss: 2.1288
Epoch 72/100, Batch 90/274, Loss: 2.0940
Epoch 72/100, Batch 100/274, Loss: 2.1171
Epoch 72/100, Batch 110/274, Loss: 2.1597
Epoch 72/100, Batch 120/274, Loss: 2.1485
Epoch 72/100, Batch 130/274, Loss: 2.1294
Epoch 72/100, Batch 140/274, Loss: 2.1359
Epoch 72/100, Batch 150/274, Loss: 2.1237
Epoch 72/100, Batch 160/274, Loss: 2.1413
Epoch 72/100, Batch 170/274, Loss: 2.2410
Epoch 72/100, Batch 180/274, Loss: 2.2051
Epoch 72/100, Batch 190/274, Loss: 2.1922
Epoch 72/100, Batch 200/274, Loss: 2.1723
Epoch 72/100, Batch 210/274, Loss: 2.1926
Epoch 72/100, Batch 220/274, Loss: 2.1268
Epoch 72/100, Batch 230/274, Loss: 2.1827
Epoch 72/100, Batch 240/274, Loss: 2.2404
Epoch 72/100, Batch 250/274, Loss: 2.2586
Epoch 72/100, Batch 260/274, Loss: 2.2774
Epoch 72/100, Batch 270/274, Loss: 2.1689
New best model with validation loss: 2.1389, perplexity: 8.49
Epoch 72/100, Loss: 2.1591, Perplexity: 8.66, Val Loss: 2.1389, Val Perplexity: 8.49, Time: 369.21s
Epoch 73/100, Batch 10/274, Loss: 2.2209
Epoch 73/100, Batch 20/274, Loss: 2.1670
Epoch 73/100, Batch 30/274, Loss: 2.1471
Epoch 73/100, Batch 40/274, Loss: 2.1503
Epoch 73/100, Batch 50/274, Loss: 2.1355
Epoch 73/100, Batch 60/274, Loss: 2.1827
Epoch 73/100, Batch 70/274, Loss: 2.1444
Epoch 73/100, Batch 80/274, Loss: 2.1172
Epoch 73/100, Batch 90/274, Loss: 2.0935
Epoch 73/100, Batch 100/274, Loss: 2.1197
Epoch 73/100, Batch 110/274, Loss: 2.1576
Epoch 73/100, Batch 120/274, Loss: 2.1500
Epoch 73/100, Batch 130/274, Loss: 2.1114
Epoch 73/100, Batch 140/274, Loss: 2.1463
Epoch 73/100, Batch 150/274, Loss: 2.1157
Epoch 73/100, Batch 160/274, Loss: 2.1274
Epoch 73/100, Batch 170/274, Loss: 2.1725
Epoch 73/100, Batch 180/274, Loss: 2.2193
Epoch 73/100, Batch 190/274, Loss: 2.2300
Epoch 73/100, Batch 200/274, Loss: 2.1304
Epoch 73/100, Batch 210/274, Loss: 2.1757
Epoch 73/100, Batch 220/274, Loss: 2.1181
Epoch 73/100, Batch 230/274, Loss: 2.1746
Epoch 73/100, Batch 240/274, Loss: 2.2389
Epoch 73/100, Batch 250/274, Loss: 2.1886
Epoch 73/100, Batch 260/274, Loss: 2.2487
Epoch 73/100, Batch 270/274, Loss: 2.2409
Epoch 73/100, Loss: 2.1579, Perplexity: 8.65, Val Loss: 2.1391, Val Perplexity: 8.49, Time: 368.26s
Epoch 74/100, Batch 10/274, Loss: 2.2068
Epoch 74/100, Batch 20/274, Loss: 2.1703
Epoch 74/100, Batch 30/274, Loss: 2.1683
Epoch 74/100, Batch 40/274, Loss: 2.1411
Epoch 74/100, Batch 50/274, Loss: 2.1287
Epoch 74/100, Batch 60/274, Loss: 2.1822
Epoch 74/100, Batch 70/274, Loss: 2.1669
Epoch 74/100, Batch 80/274, Loss: 2.1174
Epoch 74/100, Batch 90/274, Loss: 2.1009
Epoch 74/100, Batch 100/274, Loss: 2.1098
Epoch 74/100, Batch 110/274, Loss: 2.1548
Epoch 74/100, Batch 120/274, Loss: 2.1424
Epoch 74/100, Batch 130/274, Loss: 2.1099
Epoch 74/100, Batch 140/274, Loss: 2.1470
Epoch 74/100, Batch 150/274, Loss: 2.1113
Epoch 74/100, Batch 160/274, Loss: 2.1436
Epoch 74/100, Batch 170/274, Loss: 2.1788
Epoch 74/100, Batch 180/274, Loss: 2.1866
Epoch 74/100, Batch 190/274, Loss: 2.1822
Epoch 74/100, Batch 200/274, Loss: 2.1316
Epoch 74/100, Batch 210/274, Loss: 2.1733
Epoch 74/100, Batch 220/274, Loss: 2.1180
Epoch 74/100, Batch 230/274, Loss: 2.1751
Epoch 74/100, Batch 240/274, Loss: 2.1874
Epoch 74/100, Batch 250/274, Loss: 2.1836
Epoch 74/100, Batch 260/274, Loss: 2.2152
Epoch 74/100, Batch 270/274, Loss: 2.1666
Epoch 74/100, Loss: 2.1561, Perplexity: 8.64, Val Loss: 2.1410, Val Perplexity: 8.51, Time: 368.12s
Epoch 75/100, Batch 10/274, Loss: 2.2114
Epoch 75/100, Batch 20/274, Loss: 2.1674
Epoch 75/100, Batch 30/274, Loss: 2.1382
Epoch 75/100, Batch 40/274, Loss: 2.1427
Epoch 75/100, Batch 50/274, Loss: 2.1260
Epoch 75/100, Batch 60/274, Loss: 2.1787
Epoch 75/100, Batch 70/274, Loss: 2.1670
Epoch 75/100, Batch 80/274, Loss: 2.1358
Epoch 75/100, Batch 90/274, Loss: 2.1130
Epoch 75/100, Batch 100/274, Loss: 2.1028
Epoch 75/100, Batch 110/274, Loss: 2.2085
Epoch 75/100, Batch 120/274, Loss: 2.1444
Epoch 75/100, Batch 130/274, Loss: 2.1264
Epoch 75/100, Batch 140/274, Loss: 2.1384
Epoch 75/100, Batch 150/274, Loss: 2.1147
Epoch 75/100, Batch 160/274, Loss: 2.1190
Epoch 75/100, Batch 170/274, Loss: 2.2024
Epoch 75/100, Batch 180/274, Loss: 2.1850
Epoch 75/100, Batch 190/274, Loss: 2.2021
Epoch 75/100, Batch 200/274, Loss: 2.1313
Epoch 75/100, Batch 210/274, Loss: 2.1798
Epoch 75/100, Batch 220/274, Loss: 2.2143
Epoch 75/100, Batch 230/274, Loss: 2.1864
Epoch 75/100, Batch 240/274, Loss: 2.1860
Epoch 75/100, Batch 250/274, Loss: 2.1735
Epoch 75/100, Batch 260/274, Loss: 2.1955
Epoch 75/100, Batch 270/274, Loss: 2.1693
Epoch 75/100, Loss: 2.1533, Perplexity: 8.61, Val Loss: 2.1443, Val Perplexity: 8.54, Time: 368.58s
Epoch 76/100, Batch 10/274, Loss: 2.1956
Epoch 76/100, Batch 20/274, Loss: 2.1644
Epoch 76/100, Batch 30/274, Loss: 2.1304
Epoch 76/100, Batch 40/274, Loss: 2.1371
Epoch 76/100, Batch 50/274, Loss: 2.1247
Epoch 76/100, Batch 60/274, Loss: 2.2548
Epoch 76/100, Batch 70/274, Loss: 2.1296
Epoch 76/100, Batch 80/274, Loss: 2.1192
Epoch 76/100, Batch 90/274, Loss: 2.0945
Epoch 76/100, Batch 100/274, Loss: 2.1061
Epoch 76/100, Batch 110/274, Loss: 2.1778
Epoch 76/100, Batch 120/274, Loss: 2.1517
Epoch 76/100, Batch 130/274, Loss: 2.1127
Epoch 76/100, Batch 140/274, Loss: 2.1435
Epoch 76/100, Batch 150/274, Loss: 2.1084
Epoch 76/100, Batch 160/274, Loss: 2.1230
Epoch 76/100, Batch 170/274, Loss: 2.1618
Epoch 76/100, Batch 180/274, Loss: 2.1754
Epoch 76/100, Batch 190/274, Loss: 2.1880
Epoch 76/100, Batch 200/274, Loss: 2.1430
Epoch 76/100, Batch 210/274, Loss: 2.1690
Epoch 76/100, Batch 220/274, Loss: 2.1120
Epoch 76/100, Batch 230/274, Loss: 2.1759
Epoch 76/100, Batch 240/274, Loss: 2.1820
Epoch 76/100, Batch 250/274, Loss: 2.1754
Epoch 76/100, Batch 260/274, Loss: 2.1930
Epoch 76/100, Batch 270/274, Loss: 2.1742
Epoch 76/100, Loss: 2.1513, Perplexity: 8.60, Val Loss: 2.1434, Val Perplexity: 8.53, Time: 368.95s
Epoch 77/100, Batch 10/274, Loss: 2.1953
Epoch 77/100, Batch 20/274, Loss: 2.1574
Epoch 77/100, Batch 30/274, Loss: 2.1357
Epoch 77/100, Batch 40/274, Loss: 2.1361
Epoch 77/100, Batch 50/274, Loss: 2.1206
Epoch 77/100, Batch 60/274, Loss: 2.1746
Epoch 77/100, Batch 70/274, Loss: 2.1339
Epoch 77/100, Batch 80/274, Loss: 2.1104
Epoch 77/100, Batch 90/274, Loss: 2.0945
Epoch 77/100, Batch 100/274, Loss: 2.0966
Epoch 77/100, Batch 110/274, Loss: 2.1613
Epoch 77/100, Batch 120/274, Loss: 2.1380
Epoch 77/100, Batch 130/274, Loss: 2.1222
Epoch 77/100, Batch 140/274, Loss: 2.1350
Epoch 77/100, Batch 150/274, Loss: 2.1083
Epoch 77/100, Batch 160/274, Loss: 2.1132
Epoch 77/100, Batch 170/274, Loss: 2.1687
Epoch 77/100, Batch 180/274, Loss: 2.1747
Epoch 77/100, Batch 190/274, Loss: 2.1840
Epoch 77/100, Batch 200/274, Loss: 2.1290
Epoch 77/100, Batch 210/274, Loss: 2.1780
Epoch 77/100, Batch 220/274, Loss: 2.1163
Epoch 77/100, Batch 230/274, Loss: 2.1736
Epoch 77/100, Batch 240/274, Loss: 2.1851
Epoch 77/100, Batch 250/274, Loss: 2.2180
Epoch 77/100, Batch 260/274, Loss: 2.1865
Epoch 77/100, Batch 270/274, Loss: 2.1652
No improvement for 5 epochs. Early stopping.
Loaded best model with validation loss: 2.1389, perplexity: 8.49

Training visualization saved to enhanced_char_transformer_loss.png

=== Generating Text ===
Prompt: The quick brown fox
Generated: The quick brown fox, and the membership of the [[Atlantic Ocean]].

=== Players ==
* ''[[The Common Explanation]]'' (1938)
* ''[[The Book of the Common Explanation]]'' (1947)
* ''[[The Dominion of the United States of the United States|Representative]]'' (1952)
* ''[[The Membership of the World War II]]'' (1956)
* ''[[A Common Explanation of the United States]]'' (1950)
* ''[[The Great States of the United States]]'' (1953)
* ''[[The Conservative Progression of the United States]]'' (1952)

== External links ==

*
Model saved to enhanced_char_transformer_model.pt

=== Generating with Different Temperatures ===

Temperature: 0.5
Generated: The quick brown fox that plays the two plays of a common brown.

==See also==
*[[Ambrosius]]
*[[Ambrosius (play)|Ambrosius]]
*[[Ambrosius (play)]]
*[[Ambrosius]]
*[[Ambrosius]]
*[[Chemical ambrosius]]
*[[List of ambrosius ambrosius ambrosius]]
*[[Roman ambrosius ambrosius]]

==External links==
*[http://www.governments.org/ Government of Ambrosius]
*[http://www.governments.org/ Government of Ambrosius ambrosius]

{{Wikisource1911Encyclopedia}}

[[Category:Ambrosius]]
[[Category:History of Ambrosius]]

[[de:Ambrosiu

Temperature: 0.7
Generated: The quick brown fox of the Apollo was named after the [[Colony]] [[Brown (tone)|Brown]]. The [[Greek language|Greek]], the [[Poland]] are called ''[[Greek language|Greek]]'' ([[Spanish language|Spanish]]: ''[[Moon Language|Moon]]''), and the [[Greek language|Greek]] ''[[Hallow II]]''. 

In 1946, the two ''[[Denized Libraries]]'' was ranked by the children of the [[Olympic Libraries]] was also the broad [[Heritage]] in [[Elgenstein]] and the [[United Kingdom]] on the [[United Kingdom]] of the [[Roman Catholic Churc

Temperature: 0.9
Generated: The quick brown foxs of the English-report.
*[http://www.adhome.com/ ADHOMe Legend of Barbarians], a virtual country by Reading Sight Computers, who books are made in foxs of superfix at the [[New York]].
*[http://www.guide.com/ Guide Maria (ADS)]
*[http://www.adhome.com/calledge.htm The Dome]
*[http://www.anagradio.com/ The South Anagradio Maria Brown at Anagradio] (USA)
*[http://www.anagradio.com/ Anagradio London Notes Computers]

{{USA|class=Julia, Anagradio}}

[[Category:Military of Anagradio]]
[[Category:Ana
