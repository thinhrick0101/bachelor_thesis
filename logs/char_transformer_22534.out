Loading data...
Loading data from data/enwik8
Data loaded: 99621832 characters
Limiting data to first 10000000 characters for training
Vocabulary size: 2102 characters
Creating batches...
Created 274 training batches and 30 validation batches
Model Parameters: 64,624,719 trainable out of 64,624,719 total

=== Training Enhanced Character Transformer Model ===
Training on device: cuda
Using mixed precision training (FP16)
Using gradient accumulation with 4 steps
Effective batch size: 256
Epoch 1/100, Batch 10/274, Loss: 7.6217
Epoch 1/100, Batch 20/274, Loss: 7.4668
Epoch 1/100, Batch 30/274, Loss: 7.0493
Epoch 1/100, Batch 40/274, Loss: 6.7882
Epoch 1/100, Batch 50/274, Loss: 6.3851
Epoch 1/100, Batch 60/274, Loss: 6.1692
Epoch 1/100, Batch 70/274, Loss: 5.9634
Epoch 1/100, Batch 80/274, Loss: 5.8752
Epoch 1/100, Batch 90/274, Loss: 5.7329
Epoch 1/100, Batch 100/274, Loss: 5.6302
Epoch 1/100, Batch 110/274, Loss: 5.4640
Epoch 1/100, Batch 120/274, Loss: 5.4645
Epoch 1/100, Batch 130/274, Loss: 5.2957
Epoch 1/100, Batch 140/274, Loss: 5.2181
Epoch 1/100, Batch 150/274, Loss: 5.0773
Epoch 1/100, Batch 160/274, Loss: 5.0144
Epoch 1/100, Batch 170/274, Loss: 4.7857
Epoch 1/100, Batch 180/274, Loss: 4.6912
Epoch 1/100, Batch 190/274, Loss: 4.5554
Epoch 1/100, Batch 200/274, Loss: 4.4302
Epoch 1/100, Batch 210/274, Loss: 4.2640
Epoch 1/100, Batch 220/274, Loss: 4.2348
Epoch 1/100, Batch 230/274, Loss: 4.0900
Epoch 1/100, Batch 240/274, Loss: 4.0060
Epoch 1/100, Batch 250/274, Loss: 3.9667
Epoch 1/100, Batch 260/274, Loss: 3.9159
Epoch 1/100, Batch 270/274, Loss: 3.8006
New best model with validation loss: 3.8272, perplexity: 45.93
Epoch 1/100, Loss: 5.3346, Perplexity: 207.40, Val Loss: 3.8272, Val Perplexity: 45.93, Time: 1814.31s
Epoch 2/100, Batch 10/274, Loss: 3.8140
Epoch 2/100, Batch 20/274, Loss: 3.7563
Epoch 2/100, Batch 30/274, Loss: 3.7757
Epoch 2/100, Batch 40/274, Loss: 3.7508
Epoch 2/100, Batch 50/274, Loss: 3.7069
Epoch 2/100, Batch 60/274, Loss: 3.7016
Epoch 2/100, Batch 70/274, Loss: 3.6868
Epoch 2/100, Batch 80/274, Loss: 3.6832
Epoch 2/100, Batch 90/274, Loss: 3.6432
Epoch 2/100, Batch 100/274, Loss: 3.6283
Epoch 2/100, Batch 110/274, Loss: 3.5904
Epoch 2/100, Batch 120/274, Loss: 3.6433
Epoch 2/100, Batch 130/274, Loss: 3.5922
Epoch 2/100, Batch 140/274, Loss: 3.5977
Epoch 2/100, Batch 150/274, Loss: 3.5611
Epoch 2/100, Batch 160/274, Loss: 3.6071
Epoch 2/100, Batch 170/274, Loss: 3.5556
Epoch 2/100, Batch 180/274, Loss: 3.5371
Epoch 2/100, Batch 190/274, Loss: 3.5400
Epoch 2/100, Batch 200/274, Loss: 3.5196
Epoch 2/100, Batch 210/274, Loss: 3.5037
Epoch 2/100, Batch 220/274, Loss: 3.5373
Epoch 2/100, Batch 230/274, Loss: 3.5177
Epoch 2/100, Batch 240/274, Loss: 3.5283
Epoch 2/100, Batch 250/274, Loss: 3.5306
Epoch 2/100, Batch 260/274, Loss: 3.5326
Epoch 2/100, Batch 270/274, Loss: 3.4871
New best model with validation loss: 3.4822, perplexity: 32.53
Epoch 2/100, Loss: 3.6141, Perplexity: 37.12, Val Loss: 3.4822, Val Perplexity: 32.53, Time: 1799.22s
Epoch 3/100, Batch 10/274, Loss: 3.5213
Epoch 3/100, Batch 20/274, Loss: 3.4893
Epoch 3/100, Batch 30/274, Loss: 3.5084
Epoch 3/100, Batch 40/274, Loss: 3.4997
Epoch 3/100, Batch 50/274, Loss: 3.4746
Epoch 3/100, Batch 60/274, Loss: 3.5044
Epoch 3/100, Batch 70/274, Loss: 3.4769
Epoch 3/100, Batch 80/274, Loss: 3.4856
Epoch 3/100, Batch 90/274, Loss: 3.4382
Epoch 3/100, Batch 100/274, Loss: 3.4522
Epoch 3/100, Batch 110/274, Loss: 3.4500
Epoch 3/100, Batch 120/274, Loss: 3.4881
Epoch 3/100, Batch 130/274, Loss: 3.4332
Epoch 3/100, Batch 140/274, Loss: 3.4534
Epoch 3/100, Batch 150/274, Loss: 3.4277
Epoch 3/100, Batch 160/274, Loss: 3.4666
Epoch 3/100, Batch 170/274, Loss: 3.4375
Epoch 3/100, Batch 180/274, Loss: 3.4241
Epoch 3/100, Batch 190/274, Loss: 3.4349
Epoch 3/100, Batch 200/274, Loss: 3.4100
Epoch 3/100, Batch 210/274, Loss: 3.3933
Epoch 3/100, Batch 220/274, Loss: 3.4158
Epoch 3/100, Batch 230/274, Loss: 3.4158
Epoch 3/100, Batch 240/274, Loss: 3.4220
Epoch 3/100, Batch 250/274, Loss: 3.4188
Epoch 3/100, Batch 260/274, Loss: 3.4266
Epoch 3/100, Batch 270/274, Loss: 3.3816
New best model with validation loss: 3.3558, perplexity: 28.67
Epoch 3/100, Loss: 3.4504, Perplexity: 31.51, Val Loss: 3.3558, Val Perplexity: 28.67, Time: 1807.50s
Epoch 4/100, Batch 10/274, Loss: 3.4137
Epoch 4/100, Batch 20/274, Loss: 3.3903
Epoch 4/100, Batch 30/274, Loss: 3.4060
Epoch 4/100, Batch 40/274, Loss: 3.3962
Epoch 4/100, Batch 50/274, Loss: 3.3611
Epoch 4/100, Batch 60/274, Loss: 3.4062
Epoch 4/100, Batch 70/274, Loss: 3.3629
Epoch 4/100, Batch 80/274, Loss: 3.3767
Epoch 4/100, Batch 90/274, Loss: 3.3105
Epoch 4/100, Batch 100/274, Loss: 3.3385
Epoch 4/100, Batch 110/274, Loss: 3.3551
Epoch 4/100, Batch 120/274, Loss: 3.3752
Epoch 4/100, Batch 130/274, Loss: 3.3270
Epoch 4/100, Batch 140/274, Loss: 3.3406
Epoch 4/100, Batch 150/274, Loss: 3.3302
Epoch 4/100, Batch 160/274, Loss: 3.3610
Epoch 4/100, Batch 170/274, Loss: 3.3409
Epoch 4/100, Batch 180/274, Loss: 3.3399
Epoch 4/100, Batch 190/274, Loss: 3.3471
Epoch 4/100, Batch 200/274, Loss: 3.3181
Epoch 4/100, Batch 210/274, Loss: 3.3173
Epoch 4/100, Batch 220/274, Loss: 3.3176
Epoch 4/100, Batch 230/274, Loss: 3.3201
Epoch 4/100, Batch 240/274, Loss: 3.3365
Epoch 4/100, Batch 250/274, Loss: 3.3226
Epoch 4/100, Batch 260/274, Loss: 3.3327
Epoch 4/100, Batch 270/274, Loss: 3.2904
New best model with validation loss: 3.2372, perplexity: 25.46
Epoch 4/100, Loss: 3.3501, Perplexity: 28.50, Val Loss: 3.2372, Val Perplexity: 25.46, Time: 1799.21s
Epoch 5/100, Batch 10/274, Loss: 3.3088
Epoch 5/100, Batch 20/274, Loss: 3.3072
Epoch 5/100, Batch 30/274, Loss: 3.3095
Epoch 5/100, Batch 40/274, Loss: 3.2963
Epoch 5/100, Batch 50/274, Loss: 3.2609
Epoch 5/100, Batch 60/274, Loss: 3.3018
Epoch 5/100, Batch 70/274, Loss: 3.2600
Epoch 5/100, Batch 80/274, Loss: 3.2623
Epoch 5/100, Batch 90/274, Loss: 3.2284
Epoch 5/100, Batch 100/274, Loss: 3.2372
Epoch 5/100, Batch 110/274, Loss: 3.2773
Epoch 5/100, Batch 120/274, Loss: 3.2722
Epoch 5/100, Batch 130/274, Loss: 3.2309
Epoch 5/100, Batch 140/274, Loss: 3.2195
Epoch 5/100, Batch 150/274, Loss: 3.2213
Epoch 5/100, Batch 160/274, Loss: 3.2530
Epoch 5/100, Batch 170/274, Loss: 3.2624
Epoch 5/100, Batch 180/274, Loss: 3.2335
Epoch 5/100, Batch 190/274, Loss: 3.2517
Epoch 5/100, Batch 200/274, Loss: 3.2297
Epoch 5/100, Batch 210/274, Loss: 3.2451
Epoch 5/100, Batch 220/274, Loss: 3.2202
Epoch 5/100, Batch 230/274, Loss: 3.2353
Epoch 5/100, Batch 240/274, Loss: 3.2432
Epoch 5/100, Batch 250/274, Loss: 3.2332
Epoch 5/100, Batch 260/274, Loss: 3.2393
Epoch 5/100, Batch 270/274, Loss: 3.1964
New best model with validation loss: 3.1305, perplexity: 22.88
Epoch 5/100, Loss: 3.2502, Perplexity: 25.80, Val Loss: 3.1305, Val Perplexity: 22.88, Time: 1791.63s
Epoch 6/100, Batch 10/274, Loss: 3.2146
Epoch 6/100, Batch 20/274, Loss: 3.2087
Epoch 6/100, Batch 30/274, Loss: 3.2002
Epoch 6/100, Batch 40/274, Loss: 3.2036
Epoch 6/100, Batch 50/274, Loss: 3.1604
Epoch 6/100, Batch 60/274, Loss: 3.2099
Epoch 6/100, Batch 70/274, Loss: 3.1607
Epoch 6/100, Batch 80/274, Loss: 3.1643
Epoch 6/100, Batch 90/274, Loss: 3.1038
Epoch 6/100, Batch 100/274, Loss: 3.1383
Epoch 6/100, Batch 110/274, Loss: 3.1578
Epoch 6/100, Batch 120/274, Loss: 3.1731
Epoch 6/100, Batch 130/274, Loss: 3.1488
Epoch 6/100, Batch 140/274, Loss: 3.1398
Epoch 6/100, Batch 150/274, Loss: 3.1232
Epoch 6/100, Batch 160/274, Loss: 3.1664
Epoch 6/100, Batch 170/274, Loss: 3.1549
Epoch 6/100, Batch 180/274, Loss: 3.1518
Epoch 6/100, Batch 190/274, Loss: 3.1729
Epoch 6/100, Batch 200/274, Loss: 3.1253
Epoch 6/100, Batch 210/274, Loss: 3.1310
Epoch 6/100, Batch 220/274, Loss: 3.1439
Epoch 6/100, Batch 230/274, Loss: 3.1770
Epoch 6/100, Batch 240/274, Loss: 3.1573
Epoch 6/100, Batch 250/274, Loss: 3.1371
Epoch 6/100, Batch 260/274, Loss: 3.1477
Epoch 6/100, Batch 270/274, Loss: 3.1147
New best model with validation loss: 3.0353, perplexity: 20.81
Epoch 6/100, Loss: 3.1571, Perplexity: 23.50, Val Loss: 3.0353, Val Perplexity: 20.81, Time: 1804.36s
Epoch 7/100, Batch 10/274, Loss: 3.1718
Epoch 7/100, Batch 20/274, Loss: 3.1188
Epoch 7/100, Batch 30/274, Loss: 3.1223
Epoch 7/100, Batch 40/274, Loss: 3.1098
Epoch 7/100, Batch 50/274, Loss: 3.0900
Epoch 7/100, Batch 60/274, Loss: 3.1392
Epoch 7/100, Batch 70/274, Loss: 3.0733
Epoch 7/100, Batch 80/274, Loss: 3.0954
Epoch 7/100, Batch 90/274, Loss: 3.0173
Epoch 7/100, Batch 100/274, Loss: 3.0521
Epoch 7/100, Batch 110/274, Loss: 3.0873
Epoch 7/100, Batch 120/274, Loss: 3.0910
Epoch 7/100, Batch 130/274, Loss: 3.0511
Epoch 7/100, Batch 140/274, Loss: 3.0439
Epoch 7/100, Batch 150/274, Loss: 3.0414
Epoch 7/100, Batch 160/274, Loss: 3.0848
Epoch 7/100, Batch 170/274, Loss: 3.0856
Epoch 7/100, Batch 180/274, Loss: 3.0730
Epoch 7/100, Batch 190/274, Loss: 3.0962
Epoch 7/100, Batch 200/274, Loss: 3.0611
Epoch 7/100, Batch 210/274, Loss: 3.0583
Epoch 7/100, Batch 220/274, Loss: 3.0638
Epoch 7/100, Batch 230/274, Loss: 3.0785
Epoch 7/100, Batch 240/274, Loss: 3.0893
Epoch 7/100, Batch 250/274, Loss: 3.0581
Epoch 7/100, Batch 260/274, Loss: 3.0853
Epoch 7/100, Batch 270/274, Loss: 3.0479
New best model with validation loss: 2.9583, perplexity: 19.27
Epoch 7/100, Loss: 3.0803, Perplexity: 21.77, Val Loss: 2.9583, Val Perplexity: 19.27, Time: 1792.10s
Epoch 8/100, Batch 10/274, Loss: 3.0700
Epoch 8/100, Batch 20/274, Loss: 3.0581
Epoch 8/100, Batch 30/274, Loss: 3.0477
Epoch 8/100, Batch 40/274, Loss: 3.0332
Epoch 8/100, Batch 50/274, Loss: 2.9985
Epoch 8/100, Batch 60/274, Loss: 3.1415
Epoch 8/100, Batch 70/274, Loss: 3.0081
Epoch 8/100, Batch 80/274, Loss: 3.0138
Epoch 8/100, Batch 90/274, Loss: 2.9665
Epoch 8/100, Batch 100/274, Loss: 3.0106
Epoch 8/100, Batch 110/274, Loss: 3.0182
Epoch 8/100, Batch 120/274, Loss: 3.0575
Epoch 8/100, Batch 130/274, Loss: 2.9797
Epoch 8/100, Batch 140/274, Loss: 2.9793
Epoch 8/100, Batch 150/274, Loss: 2.9719
Epoch 8/100, Batch 160/274, Loss: 3.0204
Epoch 8/100, Batch 170/274, Loss: 3.0229
Epoch 8/100, Batch 180/274, Loss: 3.0111
Epoch 8/100, Batch 190/274, Loss: 3.0482
Epoch 8/100, Batch 200/274, Loss: 2.9847
Epoch 8/100, Batch 210/274, Loss: 2.9976
Epoch 8/100, Batch 220/274, Loss: 2.9970
Epoch 8/100, Batch 230/274, Loss: 3.0322
Epoch 8/100, Batch 240/274, Loss: 3.0288
Epoch 8/100, Batch 250/274, Loss: 3.0290
Epoch 8/100, Batch 260/274, Loss: 3.0295
Epoch 8/100, Batch 270/274, Loss: 2.9817
New best model with validation loss: 2.8987, perplexity: 18.15
Epoch 8/100, Loss: 3.0133, Perplexity: 20.36, Val Loss: 2.8987, Val Perplexity: 18.15, Time: 1803.38s
Epoch 9/100, Batch 10/274, Loss: 3.0131
Epoch 9/100, Batch 20/274, Loss: 2.9989
Epoch 9/100, Batch 30/274, Loss: 2.9817
Epoch 9/100, Batch 40/274, Loss: 2.9758
Epoch 9/100, Batch 50/274, Loss: 2.9408
Epoch 9/100, Batch 60/274, Loss: 3.0134
Epoch 9/100, Batch 70/274, Loss: 2.9565
Epoch 9/100, Batch 80/274, Loss: 2.9599
Epoch 9/100, Batch 90/274, Loss: 2.8917
Epoch 9/100, Batch 100/274, Loss: 2.9295
Epoch 9/100, Batch 110/274, Loss: 2.9721
Epoch 9/100, Batch 120/274, Loss: 2.9585
Epoch 9/100, Batch 130/274, Loss: 2.9240
Epoch 9/100, Batch 140/274, Loss: 2.9250
Epoch 9/100, Batch 150/274, Loss: 2.9083
Epoch 9/100, Batch 160/274, Loss: 2.9577
Epoch 9/100, Batch 170/274, Loss: 2.9689
Epoch 9/100, Batch 180/274, Loss: 2.9585
Epoch 9/100, Batch 190/274, Loss: 2.9703
